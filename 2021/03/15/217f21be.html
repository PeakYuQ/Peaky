<!DOCTYPE html><html lang="zh-Hans"><head><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description" content="There is no description"><title> | NoteBook</title><link rel="stylesheet" type="text/css" href="/peaky/css/style.css?v=1.0.0"><link rel="stylesheet" type="text/css" href="/peaky/css/dark.css?v=1.0.0"><link rel="stylesheet" type="text/css" href="//cdn.jsdelivr.net/npm/normalize.css/normalize.min.css"><link rel="stylesheet" type="text/css" href="//cdn.jsdelivr.net/npm/purecss/build/pure-min.min.css"><link rel="stylesheet" type="text/css" href="//cdn.jsdelivr.net/npm/purecss/build/grids-responsive-min.css"><link rel="stylesheet" href="//cdn.jsdelivr.net/npm/font-awesome@4.7.0/css/font-awesome.min.css"><script type="text/javascript" src="//cdn.jsdelivr.net/npm/jquery/dist/jquery.min.js"></script><link rel="icon" mask="" sizes="any" href="/peaky/favicon.ico"><link rel="Shortcut Icon" type="image/x-icon" href="/peaky/favicon.ico"><link rel="apple-touch-icon" href="/peaky/apple-touch-icon.png"><link rel="apple-touch-icon-precomposed" href="/peaky/apple-touch-icon.png"><script type="text/javascript" src="//cdn.jsdelivr.net/npm/clipboard/dist/clipboard.min.js"></script><script type="text/javascript" src="//cdn.jsdelivr.net/gh/codeseven/toastr/build/toastr.min.js"></script><link rel="stylesheet" href="//cdn.jsdelivr.net/gh/codeseven/toastr/build/toastr.min.css"><div class="darkmode-toggle">🌓</div><script>var prefersDarkMode = window.matchMedia('(prefers-color-scheme: dark)');
var toggle = document.querySelector('.darkmode-toggle');
var html = document.querySelector('html');

html.dataset.dark = localStorage.dark || prefersDarkMode.matches;

toggle.addEventListener('click', () => {
localStorage.dark = !(html.dataset.dark == 'true');
html.dataset.dark = localStorage.dark;
});</script><meta name="generator" content="Hexo 6.0.0"></head><body><div class="body_container"><div id="header"><div class="site-name"><h1 class="hidden">NoteBook</h1><a id="logo" href="/peaky/.">NoteBook</a><p class="description">Always believe that something wonderful is about to hanppen!</p></div><div id="nav-menu"><a class="current" href="/peaky/."><i class="fa fa-home"> 首页</i></a><a href="/peaky/archives/"><i class="fa fa-archive"> 归档</i></a><a href="/peaky/atom.xml"><i class="fa fa-rss"> 订阅</i></a></div></div><div class="pure-g" id="layout"><div class="pure-u-1 pure-u-md-3-4"><div class="content_container"><div class="post"><h1 class="post-title"></h1><div class="post-meta">2021-03-15<span> | </span><span class="category"><a href="/peaky/categories/RL/">RL</a></span><script src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" async></script><span id="busuanzi_container_page_pv"> | <span id="busuanzi_value_page_pv"></span><span> Hits</span></span><span class="post-time"><span class="post-meta-item-text"> | </span><span class="post-meta-item-icon"><i class="fa fa-keyboard-o"></i><span class="post-count"> 10.3k</span><span class="post-meta-item-text"> Words</span></span></span><span class="post-time"> | <span class="post-meta-item-icon"><i class="fa fa-clock-o"></i><span class="post-count"> 46</span><span class="post-meta-item-text"> Minutes</span></span></span></div><a class="disqus-comment-count" href="/peaky/2021/03/15/217f21be.html#vcomment"><span class="valine-comment-count" data-xid="/peaky/2021/03/15/217f21be.html"></span><span> Comment</span></a><div class="post-content"><h4 id="强化学习介绍"><a href="#强化学习介绍" class="headerlink" title="强化学习介绍"></a>强化学习介绍</h4><p>根据不同的分列方法可以将强化学习算法分成不同的种类：<br><strong>1.基于概率（policy-based）和基于价值（value-based）</strong></p>
<p>基于概率是强化学习中最直接的一种, 他能通过感官分析所处的环境, 直接输出下一步要采取的各种动作的概率, 然后根据概率采取行动, 所以每种动作都有可能被选中, 只是可能性不同. 而基于价值的方法输出则是所有动作的价值, 我们会根据最高价值来选着动作, 相比基于概率的方法, 基于价值的决策部分更为铁定, 毫不留情, 就选价值最高的, 而基于概率的, 即使某个动作的概率最高, 但是还是不一定会选到他.</p>
<p><img src="https://pic-20201016.oss-cn-qingdao.aliyuncs.com/img/image-20210312210405206.png" alt="image-20210312210405206"></p>
<p>其中policy-based中的典型算法有Policy Gradients，value-based的典型算法有Q-learning、SARSA、DQN，两者重合的典型模型有AC、A2C、A3C</p>
<span id="more"></span>

<p><strong>2.在线学习（on-policy）和离线学习（off-policy）</strong></p>
<p>所谓在线学习（on-policy）, 就是指我必须本人在场, 并且一定是本人边玩边学习, 学习者与环境必须产生实际的交互。</p>
<p>而离线学习（off-policy）是你可以选择自己玩, 也可以选择看着别人玩, 通过看别人玩来学习别人的行为准则。</p>
<p>on-policy的典型算法是SARSA， off-policy的典型算法是Q-learning、DQN。</p>
<p>on-policy是的学习者必学进行完一系列实际动作后才能产生样本，这样效率往往较慢。off-policy可以从以往的经验或别人的动作开学习，效率往往比较高。</p>
<p><img src="https://pic-20201016.oss-cn-qingdao.aliyuncs.com/img/image-20210312210627505.png" alt="image-20210312210627505"></p>
<p><strong>3.model-based和model-free</strong><br>可以将所有强化学习的方法分为理不理解所处环境,如果我们不尝试去理解环境, 环境给了我们什么就是什么. 我们就把这种方法叫做 model-free, 这里的 model 就是用模型来表示环境, 那理解了环境也就是学会了用一个模型来代表环境, 所以这种就是 model-based 方法.</p>
<p><img src="https://pic-20201016.oss-cn-qingdao.aliyuncs.com/img/image-20210312210824998.png" alt="image-20210312210824998"></p>
<h4 id="SARSA算法原理"><a href="#SARSA算法原理" class="headerlink" title="SARSA算法原理"></a>SARSA算法原理</h4><p>强化学习的主要功能就是让agent学习尽可能好的动作action，使其后续获得的奖励尽可能的大。</p>
<p>假设在时刻t时，处于状态长期奖励为：<br><img src="https://pic-20201016.oss-cn-qingdao.aliyuncs.com/img/2020032311212617.png" alt="在这里插入图片描述"></p>
<p><img src="https://pic-20201016.oss-cn-qingdao.aliyuncs.com/img/image-20210312211035990.png" alt="image-20210312211035990"><img src="https://pic-20201016.oss-cn-qingdao.aliyuncs.com/img/image-20210312211102633.png" alt="image-20210312211102633"></p>
<p> <img src="https://pic-20201016.oss-cn-qingdao.aliyuncs.com/img/image-20210312213327171.png" alt="image-20210312213327171"></p>
<p><img src="https://pic-20201016.oss-cn-qingdao.aliyuncs.com/img/image-20210312213401080.png" alt="image-20210312213401080"></p>
<p><img src="https://pic-20201016.oss-cn-qingdao.aliyuncs.com/img/image-20210312213418619.png" alt="image-20210312213418619"></p>
<h4 id="二、SARSA代码"><a href="#二、SARSA代码" class="headerlink" title="二、SARSA代码"></a>二、SARSA代码</h4><p>此处直接参考莫烦python的<a target="_blank" rel="noopener" href="https://morvanzhou.github.io/tutorials/machine-learning/reinforcement-learning/1-1-A-RL/">强化学习教程</a>进行代码编写，在基础上说明每一行代码的用途</p>
<p>1.environment的编写<br>首先RL需要一个环境，因为我们控制不了环境（比如下围棋时我们不不能改变棋盘的大小，何落子方式，只能只能在范围内落在线与线之间的交叉点上），这个环境是不可以改变的，因此后面的Q-learning也将沿用此环境。通常不同的问题有不同环境，我们真正需要关注的是agent即算法逻辑的编写。<br>此处以走方格为例编写一个environment</p>
<p><img src="https://pic-20201016.oss-cn-qingdao.aliyuncs.com/img/image-20210312215227492.png" alt="image-20210312215227492"></p>
<p>maze_env</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">Reinforcement learning maze example.</span></span><br><span class="line"><span class="string">Red rectangle:          explorer.</span></span><br><span class="line"><span class="string">Black rectangles:       hells       [reward = -1].</span></span><br><span class="line"><span class="string">Yellow bin circle:      paradise    [reward = +1].</span></span><br><span class="line"><span class="string">All other states:       ground      [reward = 0].</span></span><br><span class="line"><span class="string">This script is the environment part of this example. The RL is in RL_brain.py.</span></span><br><span class="line"><span class="string">View more on my tutorial page: https://morvanzhou.github.io/tutorials/</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"><span class="keyword">if</span> sys.version_info.major == <span class="number">2</span>:</span><br><span class="line">    <span class="keyword">import</span> Tkinter <span class="keyword">as</span> tk</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    <span class="keyword">import</span> tkinter <span class="keyword">as</span> tk</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">UNIT = <span class="number">40</span>   <span class="comment"># pixels</span></span><br><span class="line">MAZE_H = <span class="number">4</span>  <span class="comment"># grid height</span></span><br><span class="line">MAZE_W = <span class="number">4</span>  <span class="comment"># grid width</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Maze</span>(tk.Tk, <span class="built_in">object</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(Maze, self).__init__()</span><br><span class="line">        self.action_space = [<span class="string">&#x27;u&#x27;</span>, <span class="string">&#x27;d&#x27;</span>, <span class="string">&#x27;l&#x27;</span>, <span class="string">&#x27;r&#x27;</span>]</span><br><span class="line">        self.n_actions = <span class="built_in">len</span>(self.action_space)</span><br><span class="line">        self.title(<span class="string">&#x27;maze&#x27;</span>)</span><br><span class="line">        self.geometry(<span class="string">&#x27;&#123;0&#125;x&#123;1&#125;&#x27;</span>.<span class="built_in">format</span>(MAZE_H * UNIT, MAZE_H * UNIT))</span><br><span class="line">        self._build_maze()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_build_maze</span>(<span class="params">self</span>):</span><br><span class="line">        self.canvas = tk.Canvas(self, bg=<span class="string">&#x27;white&#x27;</span>,</span><br><span class="line">                           height=MAZE_H * UNIT,</span><br><span class="line">                           width=MAZE_W * UNIT)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># create grids</span></span><br><span class="line">        <span class="keyword">for</span> c <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, MAZE_W * UNIT, UNIT):</span><br><span class="line">            x0, y0, x1, y1 = c, <span class="number">0</span>, c, MAZE_H * UNIT</span><br><span class="line">            self.canvas.create_line(x0, y0, x1, y1)</span><br><span class="line">        <span class="keyword">for</span> r <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, MAZE_H * UNIT, UNIT):</span><br><span class="line">            x0, y0, x1, y1 = <span class="number">0</span>, r, MAZE_W * UNIT, r</span><br><span class="line">            self.canvas.create_line(x0, y0, x1, y1)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># create origin</span></span><br><span class="line">        origin = np.array([<span class="number">20</span>, <span class="number">20</span>])</span><br><span class="line"></span><br><span class="line">        <span class="comment"># hell</span></span><br><span class="line">        hell1_center = origin + np.array([UNIT * <span class="number">2</span>, UNIT])</span><br><span class="line">        self.hell1 = self.canvas.create_rectangle(</span><br><span class="line">            hell1_center[<span class="number">0</span>] - <span class="number">15</span>, hell1_center[<span class="number">1</span>] - <span class="number">15</span>,</span><br><span class="line">            hell1_center[<span class="number">0</span>] + <span class="number">15</span>, hell1_center[<span class="number">1</span>] + <span class="number">15</span>,</span><br><span class="line">            fill=<span class="string">&#x27;black&#x27;</span>)</span><br><span class="line">        <span class="comment"># hell</span></span><br><span class="line">        hell2_center = origin + np.array([UNIT, UNIT * <span class="number">2</span>])</span><br><span class="line">        self.hell2 = self.canvas.create_rectangle(</span><br><span class="line">            hell2_center[<span class="number">0</span>] - <span class="number">15</span>, hell2_center[<span class="number">1</span>] - <span class="number">15</span>,</span><br><span class="line">            hell2_center[<span class="number">0</span>] + <span class="number">15</span>, hell2_center[<span class="number">1</span>] + <span class="number">15</span>,</span><br><span class="line">            fill=<span class="string">&#x27;black&#x27;</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># create oval</span></span><br><span class="line">        oval_center = origin + UNIT * <span class="number">2</span></span><br><span class="line">        self.oval = self.canvas.create_oval(</span><br><span class="line">            oval_center[<span class="number">0</span>] - <span class="number">15</span>, oval_center[<span class="number">1</span>] - <span class="number">15</span>,</span><br><span class="line">            oval_center[<span class="number">0</span>] + <span class="number">15</span>, oval_center[<span class="number">1</span>] + <span class="number">15</span>,</span><br><span class="line">            fill=<span class="string">&#x27;yellow&#x27;</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># create red rect</span></span><br><span class="line">        self.rect = self.canvas.create_rectangle(</span><br><span class="line">            origin[<span class="number">0</span>] - <span class="number">15</span>, origin[<span class="number">1</span>] - <span class="number">15</span>,</span><br><span class="line">            origin[<span class="number">0</span>] + <span class="number">15</span>, origin[<span class="number">1</span>] + <span class="number">15</span>,</span><br><span class="line">            fill=<span class="string">&#x27;red&#x27;</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># pack all</span></span><br><span class="line">        self.canvas.pack()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">reset</span>(<span class="params">self</span>):</span><br><span class="line">        self.update()</span><br><span class="line">        time.sleep(<span class="number">0.5</span>)</span><br><span class="line">        self.canvas.delete(self.rect)</span><br><span class="line">        origin = np.array([<span class="number">20</span>, <span class="number">20</span>])</span><br><span class="line">        self.rect = self.canvas.create_rectangle(</span><br><span class="line">            origin[<span class="number">0</span>] - <span class="number">15</span>, origin[<span class="number">1</span>] - <span class="number">15</span>,</span><br><span class="line">            origin[<span class="number">0</span>] + <span class="number">15</span>, origin[<span class="number">1</span>] + <span class="number">15</span>,</span><br><span class="line">            fill=<span class="string">&#x27;red&#x27;</span>)</span><br><span class="line">        <span class="comment"># return observation</span></span><br><span class="line">        <span class="keyword">return</span> self.canvas.coords(self.rect)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">step</span>(<span class="params">self, action</span>):</span><br><span class="line">        s = self.canvas.coords(self.rect)</span><br><span class="line">        base_action = np.array([<span class="number">0</span>, <span class="number">0</span>])</span><br><span class="line">        <span class="keyword">if</span> action == <span class="number">0</span>:   <span class="comment"># up</span></span><br><span class="line">            <span class="keyword">if</span> s[<span class="number">1</span>] &gt; UNIT:</span><br><span class="line">                base_action[<span class="number">1</span>] -= UNIT</span><br><span class="line">        <span class="keyword">elif</span> action == <span class="number">1</span>:   <span class="comment"># down</span></span><br><span class="line">            <span class="keyword">if</span> s[<span class="number">1</span>] &lt; (MAZE_H - <span class="number">1</span>) * UNIT:</span><br><span class="line">                base_action[<span class="number">1</span>] += UNIT</span><br><span class="line">        <span class="keyword">elif</span> action == <span class="number">2</span>:   <span class="comment"># right</span></span><br><span class="line">            <span class="keyword">if</span> s[<span class="number">0</span>] &lt; (MAZE_W - <span class="number">1</span>) * UNIT:</span><br><span class="line">                base_action[<span class="number">0</span>] += UNIT</span><br><span class="line">        <span class="keyword">elif</span> action == <span class="number">3</span>:   <span class="comment"># left</span></span><br><span class="line">            <span class="keyword">if</span> s[<span class="number">0</span>] &gt; UNIT:</span><br><span class="line">                base_action[<span class="number">0</span>] -= UNIT</span><br><span class="line"></span><br><span class="line">        self.canvas.move(self.rect, base_action[<span class="number">0</span>], base_action[<span class="number">1</span>])  <span class="comment"># move agent</span></span><br><span class="line"></span><br><span class="line">        s_ = self.canvas.coords(self.rect)  <span class="comment"># next state</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># reward function</span></span><br><span class="line">        <span class="keyword">if</span> s_ == self.canvas.coords(self.oval):</span><br><span class="line">            reward = <span class="number">1</span></span><br><span class="line">            done = <span class="literal">True</span></span><br><span class="line">            s_ = <span class="string">&#x27;terminal&#x27;</span></span><br><span class="line">        <span class="keyword">elif</span> s_ <span class="keyword">in</span> [self.canvas.coords(self.hell1), self.canvas.coords(self.hell2)]:</span><br><span class="line">            reward = -<span class="number">1</span></span><br><span class="line">            done = <span class="literal">True</span></span><br><span class="line">            s_ = <span class="string">&#x27;terminal&#x27;</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            reward = <span class="number">0</span></span><br><span class="line">            done = <span class="literal">False</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> s_, reward, done</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">render</span>(<span class="params">self</span>):</span><br><span class="line">        time.sleep(<span class="number">0.1</span>)</span><br><span class="line">        self.update()</span><br><span class="line">        </span><br><span class="line"><span class="keyword">def</span> <span class="title function_">update</span>():</span><br><span class="line">    <span class="keyword">for</span> t <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">10</span>):</span><br><span class="line">        s = env.reset()</span><br><span class="line">        <span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">            env.render()</span><br><span class="line">            a = <span class="number">1</span></span><br><span class="line">            s, r, done = env.step(a)</span><br><span class="line">            <span class="keyword">if</span> done:</span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    env = Maze()</span><br><span class="line">    env.after(<span class="number">100</span>, update)</span><br><span class="line">    env.mainloop()</span><br></pre></td></tr></table></figure>

<p><strong>agent</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> maze_env <span class="keyword">import</span> Maze    <span class="comment">#即为上面的environment</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"></span><br><span class="line"><span class="comment">#RL的父类定义</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">RL</span>(<span class="title class_ inherited__">object</span>):</span><br><span class="line">    <span class="comment">#初始化</span></span><br><span class="line">    <span class="comment">#actions为可选动作， learning_rate为学习率，reward_decay为传递奖励是的递减系数gamma，1-e_greed为随机选择其他动作的概率</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, actions, learning_rate=<span class="number">0.01</span>, reward_decay=<span class="number">0.9</span>, e_greedy=<span class="number">0.9</span></span>):</span><br><span class="line">        self.actions = actions</span><br><span class="line">        self.lr = learning_rate</span><br><span class="line">        self.gamma = reward_decay</span><br><span class="line">        self.epsilon = e_greedy</span><br><span class="line">        <span class="comment">#初始化qtable，行为observation的state， 列为当前状态可以选择的action（对于所有列，可以选择的action一样）</span></span><br><span class="line">        self.q_table = pd.DataFrame(columns = self.actions, dtype=np.float64)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">choose_action</span>(<span class="params">self, observation</span>):</span><br><span class="line">        self.check_state_exist(observation)  <span class="comment">#检查当前状态是否存在，不存在就添加这个状态</span></span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span> np.random.uniform() &lt; self.epsilon:</span><br><span class="line">            state_action = self.q_table.loc[observation, :]  <span class="comment">#找到当前状态可以选择的动作</span></span><br><span class="line">            <span class="comment">#由于初始化或更新后一个状态下的动作值可能是相同的，为了避免每次都选择相同动作，用random.choice在值最大的action中损及选择一个</span></span><br><span class="line">            action = np.random.choice(state_action[state_action==np.<span class="built_in">max</span>(state_action)].index)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            action = np.random.choice(self.actions)   <span class="comment">#0.1的几率随机选择动作</span></span><br><span class="line">        <span class="keyword">return</span> action</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">check_state_exist</span>(<span class="params">self, state</span>):</span><br><span class="line">        <span class="keyword">if</span> state <span class="keyword">not</span> <span class="keyword">in</span> self.q_table.index:</span><br><span class="line">            <span class="comment">#若找不到该obversation的转态，则添加该状态到新的qtable</span></span><br><span class="line">            <span class="comment">#新的state的动作的q初始值赋值为0，列名为dataframe的列名，index为state</span></span><br><span class="line">            self.q_table = self.q_table.append(pd.Series([<span class="number">0</span>]*<span class="built_in">len</span>(self.actions), index=self.q_table.columns, name=state))</span><br><span class="line">    </span><br><span class="line">    <span class="comment">#不同方式的学习方法不同，用可变参数，直接pass</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">learning</span>(<span class="params">self, *args</span>):</span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">SarsaTable</span>(<span class="title class_ inherited__">RL</span>):  <span class="comment">#继承上面的RL</span></span><br><span class="line">    <span class="comment">#初始化</span></span><br><span class="line">    <span class="comment">#参数自己定义，含义继承父类RL</span></span><br><span class="line">    <span class="comment">#类方法choose_action、check_state_exist自动继承RL，参数不变</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, actions, learning_rate=<span class="number">0.01</span>, reward_decay=<span class="number">0.9</span>, e_greedy=<span class="number">0.9</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(SarsaTable, self).__init__(actions, learning_rate, reward_decay, e_greedy)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">learning</span>(<span class="params">self, s, a,r, s_, a_</span>):</span><br><span class="line">        self.check_state_exist(s_)   <span class="comment">#检查动作后状态s_是否存在</span></span><br><span class="line">        </span><br><span class="line">        q_old = self.q_table.loc[s, a]  <span class="comment">#旧的q[s,a]值</span></span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span> s_!=<span class="string">&#x27;terminal&#x27;</span>:</span><br><span class="line">            <span class="comment">#取下个状态s_和动作a_下q值</span></span><br><span class="line">            q_predict = self.q_table.loc[s_, a_]</span><br><span class="line">            q_new = r+self.gamma*q_predict   <span class="comment">#计算新的值</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            q_new = r</span><br><span class="line">        </span><br><span class="line">        self.q_table.loc[s,a] = q_old - self.lr*(q_new - q_old)    <span class="comment">#根据更新公式更新，类似于梯度下降</span></span><br><span class="line"> </span><br><span class="line"><span class="keyword">def</span> <span class="title function_">update</span>():</span><br><span class="line">    <span class="keyword">for</span> episode <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">100</span>):</span><br><span class="line">        <span class="comment">#初始化环境</span></span><br><span class="line">        observation = env.reset()</span><br><span class="line">        </span><br><span class="line">        <span class="comment">#根据当前状态选行为</span></span><br><span class="line">        action = RL.choose_action(<span class="built_in">str</span>(observation))</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">            <span class="comment"># 刷新环境</span></span><br><span class="line">            env.render()</span><br><span class="line">            </span><br><span class="line">            <span class="comment"># 在环境中采取行为, 获得下一个 state_ (obervation_), reward, 和是否终止</span></span><br><span class="line">            observation_, reward, done = env.step(action)</span><br><span class="line">            </span><br><span class="line">            <span class="comment">#根据observation_选择observation_下应该选择的动作action_</span></span><br><span class="line">            action_ = RL.choose_action(<span class="built_in">str</span>(observation_))</span><br><span class="line">            </span><br><span class="line">            <span class="comment">#从当前状态state，当前动作action，奖励r，执行动作后state_，state_下的action_,(s,a,r,s,a)</span></span><br><span class="line">            RL.learning(<span class="built_in">str</span>(observation), action, reward, <span class="built_in">str</span>(observation_), action_)</span><br><span class="line">            </span><br><span class="line">            <span class="comment"># 将下一个当成下一步的 state (observation) and action。</span></span><br><span class="line">            <span class="comment">#与qlearning的却别是sarsa在observation_下真正执行了动作action_，供下次使用</span></span><br><span class="line">            <span class="comment">#而qlearning中下次状态observation_时还要重新选择action_</span></span><br><span class="line">            observation = observation_</span><br><span class="line">            action = action_</span><br><span class="line">            </span><br><span class="line">             <span class="comment"># 终止时跳出循环</span></span><br><span class="line">        	<span class="keyword">if</span> done:</span><br><span class="line">            	<span class="keyword">break</span></span><br><span class="line">        </span><br><span class="line">    <span class="comment"># 大循环完毕</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;game over&#x27;</span>)</span><br><span class="line">    env.destroy()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    env = Maze()</span><br><span class="line">    </span><br><span class="line">    <span class="comment">#Sarsa和SarsaLambda的调用方式一模一样</span></span><br><span class="line">    <span class="comment">#RL = SarsaTable(actions=list(range(env.n_actions)))</span></span><br><span class="line">    RL = SarsaLambdaTable(actions=<span class="built_in">list</span>(<span class="built_in">range</span>(env.n_actions)))</span><br><span class="line"></span><br><span class="line">    env.after(<span class="number">100</span>, update)</span><br><span class="line">    env.mainloop()</span><br></pre></td></tr></table></figure>



<h4 id="Q-learning、"><a href="#Q-learning、" class="headerlink" title="Q-learning、"></a>Q-learning、</h4><p><img src="https://pic-20201016.oss-cn-qingdao.aliyuncs.com/img/image-20210315201907446.png" alt="image-20210315201907446"></p>
<p><img src="https://pic-20201016.oss-cn-qingdao.aliyuncs.com/img/20200323154100203.png" alt="在这里插入图片描述"></p>
<p><img src="https://pic-20201016.oss-cn-qingdao.aliyuncs.com/img/image-20210315202102199.png" alt="image-20210315202102199"></p>
<p>用同一个env</p>
<p>qlearnning-agent</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> maze_env <span class="keyword">import</span> Maze</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># RL的父类定义</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">RL</span>(<span class="title class_ inherited__">object</span>):</span><br><span class="line">    <span class="comment"># 初始化</span></span><br><span class="line">    <span class="comment"># actions为可选动作， learning_rate为学习率，reward_decay为传递奖励是的递减系数gamma，1-e_greed为随机选择其他动作的概率</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, actions, learning_rate=<span class="number">0.01</span>, reward_decay=<span class="number">0.9</span>, e_greedy=<span class="number">0.9</span></span>):</span><br><span class="line">        self.actions = actions</span><br><span class="line">        self.lr = learning_rate</span><br><span class="line">        self.gamma = reward_decay</span><br><span class="line">        self.epsilon = e_greedy</span><br><span class="line">        <span class="comment"># 初始化qtable，行为observation的state， 列为当前状态可以选择的action（对于所有列，可以选择的action一样）</span></span><br><span class="line">        self.q_table = pd.DataFrame(columns=self.actions, dtype=np.float64)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">choose_action</span>(<span class="params">self, observation</span>):</span><br><span class="line">        self.check_state_exist(observation)  <span class="comment"># 检查当前状态是否存在，不存在就添加这个状态</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> np.random.uniform() &lt; self.epsilon:</span><br><span class="line">            state_action = self.q_table.loc[observation, :]  <span class="comment"># 找到当前状态可以选择的动作</span></span><br><span class="line">            <span class="comment"># 由于初始化或更新后一个状态下的动作值可能是相同的，为了避免每次都选择相同动作，用random.choice在值最大的action中损及选择一个</span></span><br><span class="line">            action = np.random.choice(state_action[state_action == np.<span class="built_in">max</span>(state_action)].index)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            action = np.random.choice(self.actions)  <span class="comment"># 0.1的几率随机选择动作</span></span><br><span class="line">        <span class="keyword">return</span> action</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">check_state_exist</span>(<span class="params">self, state</span>):</span><br><span class="line">        <span class="keyword">if</span> state <span class="keyword">not</span> <span class="keyword">in</span> self.q_table.index:</span><br><span class="line">            <span class="comment"># 若找不到该obversation的转态，则添加该状态到新的qtable</span></span><br><span class="line">            <span class="comment"># 新的state的动作的q初始值赋值为0，列名为dataframe的列名，index为state</span></span><br><span class="line">            self.q_table = self.q_table.append(</span><br><span class="line">                pd.Series([<span class="number">0</span>] * <span class="built_in">len</span>(self.actions), index=self.q_table.columns, name=state))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 不同方式的学习方法不同，用可变参数，直接pass</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">learning</span>(<span class="params">self, *args</span>):</span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># QLearning继承RL</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">QLearningTable</span>(<span class="title class_ inherited__">RL</span>):</span><br><span class="line">    <span class="comment"># 初始化</span></span><br><span class="line">    <span class="comment"># 参数自己定义，含义继承父类RL</span></span><br><span class="line">    <span class="comment"># 类方法choose_action、check_state_exist自动继承RL，参数不变</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, actions, learning_rate=<span class="number">0.01</span>, reward_decay=<span class="number">0.9</span>, e_greedy=<span class="number">0.9</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(QLearningTable, self).__init__(actions, learning_rate, reward_decay, e_greedy)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 根绝当前观察状态s，选择动作a，选择动作后的奖励r，和执行动作后的状态s_，来更新qtable</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">learning</span>(<span class="params">self, s, a, r, s_</span>):</span><br><span class="line">        self.check_state_exist(s_)  <span class="comment"># 检查动作后状态s_是否存在</span></span><br><span class="line"></span><br><span class="line">        q_old = self.q_table.loc[s, a]  <span class="comment"># 旧的q[s,a]值</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> s_ != <span class="string">&#x27;terminal&#x27;</span>:</span><br><span class="line">            <span class="comment"># 下个状态下最大的值</span></span><br><span class="line">            max_s_ = self.q_table.loc[s_, :].<span class="built_in">max</span>()</span><br><span class="line">            q_new = r + self.gamma * max_s_  <span class="comment"># 计算新的值</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            q_new = r</span><br><span class="line"></span><br><span class="line">        self.q_table.loc[s, a] = q_old - self.lr * (q_new - q_old)  <span class="comment"># 根据更新公式更新，类似于梯度下降</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">update</span>():</span><br><span class="line">    <span class="keyword">for</span> episode <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">100</span>):</span><br><span class="line">        <span class="comment"># 初始化 state 的观测值</span></span><br><span class="line">        observation = env.reset()  <span class="comment"># 每轮训练都要初始化观测值，即回到原点状态</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">            env.render()</span><br><span class="line"></span><br><span class="line">            <span class="comment"># RL 大脑根据 state 的观测值挑选 action</span></span><br><span class="line">            action = RL.choose_action(<span class="built_in">str</span>(observation))  <span class="comment"># qlearning采用greeed方法，选择q值最大的action</span></span><br><span class="line"></span><br><span class="line">            <span class="comment"># 探索者在环境中实施这个 action, 并得到环境返回的下一个 state 观测值, reward 和 done (是否是掉下地狱或者升上天堂)</span></span><br><span class="line">            <span class="comment"># 是根据当前选择动作，观察到的采取动作后的状态和奖励</span></span><br><span class="line">            observation_, reward, done = env.step(action)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># RL 从这个序列 (state, action, reward, state_) 中学习</span></span><br><span class="line">            <span class="comment"># 根绝旧observation的q值，和采取动作，以及奖励和采取动作后的observation_的最大q值进行更新</span></span><br><span class="line">            RL.learning(<span class="built_in">str</span>(observation), action, reward, <span class="built_in">str</span>(observation_))</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 将下一个 state 的值传到下一次循环</span></span><br><span class="line">            observation = observation_</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> done:</span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 结束游戏并关闭窗口</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;game over&#x27;</span>)</span><br><span class="line">    env.destroy()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    <span class="comment"># 定义环境 env 和 RL 方式</span></span><br><span class="line">    env = Maze()</span><br><span class="line">    RL = QLearningTable(actions=<span class="built_in">list</span>(<span class="built_in">range</span>(env.n_actions)))</span><br><span class="line">    <span class="comment"># 开始可视化环境 env</span></span><br><span class="line">    env.after(<span class="number">100</span>, update)</span><br><span class="line">    env.mainloop()</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>唯一不同是on -&gt;off</p>
<p>并未真正执行a’，而是选择所有a中Q值最大的一项；</p>
<h5 id="DQN"><a href="#DQN" class="headerlink" title="DQN"></a>DQN</h5><p><img src="https://pic-20201016.oss-cn-qingdao.aliyuncs.com/img/image-20210315202734362.png" alt="image-20210315202734362"></p>
<p><img src="https://pic-20201016.oss-cn-qingdao.aliyuncs.com/img/image-20210315203248027.png" alt="image-20210315203248027"></p>
<p><img src="https://pic-20201016.oss-cn-qingdao.aliyuncs.com/img/image-20210315203406102.png" alt="image-20210315203406102"></p>
<p>DQN的算法</p>
<p><img src="https://pic-20201016.oss-cn-qingdao.aliyuncs.com/img/20200323172714471.png" alt="在这里插入图片描述"></p>
<p><img src="https://pic-20201016.oss-cn-qingdao.aliyuncs.com/img/image-20210315204554812.png" alt="image-20210315204554812"></p>
<h5 id="学习策略：observation"><a href="#学习策略：observation" class="headerlink" title="学习策略：observation"></a>学习策略：observation</h5><p><img src="https://pic-20201016.oss-cn-qingdao.aliyuncs.com/img/image-20210315213257698.png" alt="image-20210315213257698"></p>
<p><strong>更新方式一</strong>：更新慢，需要将所有的动作执行完才能更新</p>
<p><img src="https://pic-20201016.oss-cn-qingdao.aliyuncs.com/img/image-20210315205418821.png" alt="image-20210315205418821"></p>
<p><strong><img src="https://pic-20201016.oss-cn-qingdao.aliyuncs.com/img/20200323190859264.png" alt="在这里插入图片描述">更新方式二</strong>：</p>
<p><img src="https://pic-20201016.oss-cn-qingdao.aliyuncs.com/img/20200323191743435.png" alt="在这里插入图片描述"></p>
<p><img src="https://pic-20201016.oss-cn-qingdao.aliyuncs.com/img/image-20210315211111470.png" alt="image-20210315211111470"></p>
<p><img src="https://pic-20201016.oss-cn-qingdao.aliyuncs.com/img/image-20210315211131717.png" alt="image-20210315211131717"></p>
<p><img src="https://pic-20201016.oss-cn-qingdao.aliyuncs.com/img/image-20210315221326359.png" alt="image-20210315221326359"></p>
<p><img src="https://pic-20201016.oss-cn-qingdao.aliyuncs.com/img/image-20210315221342578.png" alt="image-20210315221342578"></p>
<p>但上面算法中，也有一些与Q-learning不同的地方，这是使DQN变得更加有效和技巧：</p>
<p><img src="https://pic-20201016.oss-cn-qingdao.aliyuncs.com/img/image-20210315221711848.png" alt="image-20210315221711848"></p>
<p><img src="https://pic-20201016.oss-cn-qingdao.aliyuncs.com/img/image-20210315221729557.png" alt="image-20210315221729557"></p>
<h4 id="Dobule-DQN"><a href="#Dobule-DQN" class="headerlink" title="Dobule DQN"></a>Dobule DQN</h4><p><img src="https://pic-20201016.oss-cn-qingdao.aliyuncs.com/img/image-20210316191813210.png" alt="image-20210316191813210"></p>
<p><img src="https://pic-20201016.oss-cn-qingdao.aliyuncs.com/img/image-20210316193812638.png" alt="image-20210316193812638"></p>
<p><img src="https://pic-20201016.oss-cn-qingdao.aliyuncs.com/img/image-20210316193825686.png" alt="image-20210316193825686"></p>
<p><img src="https://pic-20201016.oss-cn-qingdao.aliyuncs.com/img/image-20210316193923094.png" alt="image-20210316193923094"></p>
<p><img src="https://pic-20201016.oss-cn-qingdao.aliyuncs.com/img/image-20210316193946037.png" alt="image-20210316193946037"></p>
<p><img src="https://pic-20201016.oss-cn-qingdao.aliyuncs.com/img/image-20210316194102502.png" alt="image-20210316194102502"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Double_DeepQNetwork</span>(<span class="title class_ inherited__">object</span>):</span><br><span class="line">    <span class="comment">#replace_target_iter为更新target network的步数，防止target network和eval network差别过大</span></span><br><span class="line">    <span class="comment">#memory_size为buffer储存记忆上线，方便使用以前记忆学习</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, n_actions, n_features,learning_rate=<span class="number">0.01</span>,reward_decay=<span class="number">0.9</span>,e_greedy=<span class="number">0.9</span>,replace_target_iter=<span class="number">300</span>,memory_size=<span class="number">500</span>,batch_size=<span class="number">32</span>,e_greedy_increment=<span class="literal">None</span>,output_graph=<span class="literal">False</span></span>):</span><br><span class="line">        self.n_actions = n_actions</span><br><span class="line">        self.n_features = n_features</span><br><span class="line">        self.lr = learning_rate</span><br><span class="line">        self.gamma = reward_decay</span><br><span class="line">        self.epsilon_max = e_greedy     <span class="comment"># epsilon后面奖励对前面的递减参数</span></span><br><span class="line">        self.replace_target_iter = replace_target_iter  <span class="comment"># 更换 target_net 的步数</span></span><br><span class="line">        self.memory_size = memory_size  <span class="comment"># 记忆上限</span></span><br><span class="line">        self.batch_size = batch_size    <span class="comment"># 每次更新时从 memory 里面取多少记忆出来</span></span><br><span class="line">        self.epsilon_increment = e_greedy_increment <span class="comment"># epsilon 的增量</span></span><br><span class="line">        <span class="comment">#epsilon = 0等于0时，后面的奖励创传不到前面，前面的状态就开启随机探索模式</span></span><br><span class="line">        self.epsilon = <span class="number">0</span> <span class="keyword">if</span> e_greedy_increment <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span> <span class="keyword">else</span> self.epsilon_max <span class="comment"># 是否开启探索模式, 并逐步减少探索次数</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 记录学习次数 (用于判断是否更换 target_net 参数)</span></span><br><span class="line">        self.learn_step_counter = <span class="number">0</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 初始化全 0 记忆 [s, a, r, s_]， 实际上feature为状态的维度，n_features*2分别记录s和s_，+2记录a和r</span></span><br><span class="line">        self.memory = np.zeros((self.memory_size, n_features*<span class="number">2</span>+<span class="number">2</span>))</span><br><span class="line">        </span><br><span class="line">        self._build_net()</span><br><span class="line">        </span><br><span class="line">        <span class="comment">#替换 target net 的参数</span></span><br><span class="line">        t_params = tf.get_collection(<span class="string">&#x27;target_net_params&#x27;</span>)  <span class="comment">#提取 target_net 的参数</span></span><br><span class="line">        e_params = tf.get_collection(<span class="string">&#x27;eval_net_params&#x27;</span>)   <span class="comment"># 提取  eval_net 的参数</span></span><br><span class="line">        <span class="comment">#将eval_network中每一个variable的值赋值给target network的对应变量</span></span><br><span class="line">        self.replace_target_op = [tf.assign(t, e) <span class="keyword">for</span> t, e <span class="keyword">in</span> <span class="built_in">zip</span>(t_params, e_params)] <span class="comment">#更新 target_net 参数</span></span><br><span class="line">        </span><br><span class="line">        </span><br><span class="line">        self.sess = tf.Session()</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span> output_graph:</span><br><span class="line">            tf.summary.FileWriter(<span class="string">&quot;logs/&quot;</span>, self.sess.graph)</span><br><span class="line">        </span><br><span class="line">        self.sess.run(tf.global_variables_initializer())</span><br><span class="line">        <span class="comment">#用于记录# 记录所有 cost 变化</span></span><br><span class="line">        self.cost_his = []</span><br><span class="line">    </span><br><span class="line">    <span class="comment">#李宏毅老师克重的relpay buffer，通过以往的记忆中不断训练</span></span><br><span class="line">    <span class="comment">#这是DQN变为off-policy的核心</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">store_transition</span>(<span class="params">self, s, a, r, s_</span>):</span><br><span class="line">        <span class="comment">#如果DeepQNetwork中定义了memory_counter，进行记忆存储</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> <span class="built_in">hasattr</span>(self, <span class="string">&#x27;memory_counter&#x27;</span>):</span><br><span class="line">            self.memory_counter = <span class="number">0</span></span><br><span class="line">        <span class="comment">#记录一条 [s, a, r, s_] 记录</span></span><br><span class="line">        transition = np.hstack((s, [a, r], s_))</span><br><span class="line">        </span><br><span class="line">        <span class="comment">#总 memory 大小是固定的, 如果超出总大小, 旧 memory 就被新 memory 替换</span></span><br><span class="line">        index = self.memory_counter % self.memory_size  <span class="comment">#类似hashmap赋值思想</span></span><br><span class="line">        self.memory[index, :] = transition  <span class="comment">#进行替换</span></span><br><span class="line">        </span><br><span class="line">        self.memory_counter += <span class="number">1</span></span><br><span class="line">        </span><br><span class="line">    <span class="comment">#建立神经网络</span></span><br><span class="line">    <span class="comment">#此处建立两个申请网络，一个为target network，用于得到q现实。一个为eval_network，用于得到q估计</span></span><br><span class="line">    <span class="comment">#target network和eval_network结构一样，target network用比较老的参数，eval_network为真正训练的神经网络</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_build_net</span>(<span class="params">self</span>):</span><br><span class="line">        tf.reset_default_graph()  <span class="comment">#清空计算图</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment">#创建eval神经网络,及时提升参数</span></span><br><span class="line">        self.s = tf.placeholder(tf.float32, [<span class="literal">None</span>, self.n_features], name=<span class="string">&#x27;s&#x27;</span>)  <span class="comment"># 用来接收 observation，即神经网络的输入</span></span><br><span class="line">        self.q_target = tf.placeholder(tf.float32, [<span class="literal">None</span>, self.n_actions], name=<span class="string">&#x27;Q_target&#x27;</span>) <span class="comment"># q_target的值, 这个之后会通过计算得到，神经网络的输出</span></span><br><span class="line">        <span class="comment">#eval_net域下的变量</span></span><br><span class="line">        <span class="keyword">with</span> tf.variable_scope(<span class="string">&#x27;eval_net&#x27;</span>):</span><br><span class="line">            <span class="comment">#c_names用于在一定步数之后更新target network</span></span><br><span class="line">            <span class="comment">#GLOBAL_VARIABLES作用是collection默认加入所有的Variable对象，用于共享</span></span><br><span class="line">            c_names = [<span class="string">&#x27;eval_net_params&#x27;</span>, tf.GraphKeys.GLOBAL_VARIABLES]</span><br><span class="line">            n_l1 = <span class="number">10</span>  <span class="comment">#n_l1为network隐藏层神经元的个数 </span></span><br><span class="line">            w_initializer = tf.random_normal_initializer(<span class="number">0.</span>,<span class="number">0.3</span>)  </span><br><span class="line">            b_initializer = tf.constant_initializer(<span class="number">0.1</span>)</span><br><span class="line">            </span><br><span class="line">            <span class="comment">#eval_network第一层全连接神经网络</span></span><br><span class="line">            <span class="keyword">with</span> tf.variable_scope(<span class="string">&#x27;l1&#x27;</span>):</span><br><span class="line">                w1 = tf.get_variable(<span class="string">&#x27;w1&#x27;</span>, [self.n_features, n_l1], initializer=w_initializer, collections=c_names)</span><br><span class="line">                b1 = tf.get_variable(<span class="string">&#x27;b1&#x27;</span>, [<span class="number">1</span>, n_l1], initializer=b_initializer, collections=c_names)</span><br><span class="line">                l1 = tf.nn.relu(tf.matmul(self.s, w1)+b1)</span><br><span class="line">            </span><br><span class="line">            <span class="comment">#eval_network第二层全连接神经网络</span></span><br><span class="line">            <span class="keyword">with</span> tf.variable_scope(<span class="string">&#x27;l1&#x27;</span>):</span><br><span class="line">                w2 = tf.get_variable(<span class="string">&#x27;w2&#x27;</span>, [n_l1, self.n_actions], initializer=w_initializer, collections=c_names)</span><br><span class="line">                b2 = tf.get_variable(<span class="string">&#x27;b2&#x27;</span>, [<span class="number">1</span>, self.n_actions], initializer=b_initializer, collections=c_names)</span><br><span class="line">                <span class="comment">#求出q估计值，长度为n_actions的向量</span></span><br><span class="line">                self.q_eval = tf.matmul(l1, w2) + b2</span><br><span class="line">            </span><br><span class="line">        <span class="keyword">with</span> tf.variable_scope(<span class="string">&#x27;loss&#x27;</span>): <span class="comment"># 求误差</span></span><br><span class="line">            <span class="comment">#使用平方误差</span></span><br><span class="line">            self.loss = tf.reduce_mean(tf.squared_difference(self.q_target, self.q_eval))</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">with</span> tf.variable_scope(<span class="string">&#x27;train&#x27;</span>):    <span class="comment"># 梯度下降</span></span><br><span class="line">            optimizer = tf.train.RMSPropOptimizer(self.lr)</span><br><span class="line">            self._train_op = optimizer.minimize(self.loss)</span><br><span class="line">        </span><br><span class="line">        <span class="comment">#创建target network，输入选择一个action后的状态s_,输出q_target</span></span><br><span class="line">        self.s_ = tf.placeholder(tf.float32, [<span class="literal">None</span>, self.n_features], name=<span class="string">&#x27;s_&#x27;</span>)    <span class="comment"># 接收下个 observation</span></span><br><span class="line">        <span class="keyword">with</span> tf.variable_scope(<span class="string">&#x27;target_net&#x27;</span>):</span><br><span class="line">            c_names = [<span class="string">&#x27;target_net_params&#x27;</span>, tf.GraphKeys.GLOBAL_VARIABLES]</span><br><span class="line">            </span><br><span class="line">            <span class="comment"># target_net 的第一层fc， collections 是在更新 target_net 参数时会用到</span></span><br><span class="line">            <span class="keyword">with</span> tf.variable_scope(<span class="string">&#x27;l1&#x27;</span>):</span><br><span class="line">                w1 = tf.get_variable(<span class="string">&#x27;w1&#x27;</span>, [self.n_features, n_l1], initializer=w_initializer, collections=c_names)</span><br><span class="line">                b1 = tf.get_variable(<span class="string">&#x27;b1&#x27;</span>, [<span class="number">1</span>, n_l1], initializer=b_initializer, collections=c_names)</span><br><span class="line">                l1 = tf.nn.relu(tf.matmul(self.s_, w1) + b1)</span><br><span class="line">                </span><br><span class="line">            <span class="comment"># target_net 的第二层fc</span></span><br><span class="line">            <span class="keyword">with</span> tf.variable_scope(<span class="string">&#x27;l2&#x27;</span>):</span><br><span class="line">                w2 = tf.get_variable(<span class="string">&#x27;w2&#x27;</span>, [n_l1, self.n_actions], initializer=w_initializer, collections=c_names)</span><br><span class="line">                b2 = tf.get_variable(<span class="string">&#x27;b2&#x27;</span>, [<span class="number">1</span>, self.n_actions], initializer=b_initializer, collections=c_names)</span><br><span class="line">                <span class="comment">#申请网络输出</span></span><br><span class="line">                self.q_next = tf.matmul(l1, w2) + b2</span><br><span class="line">            </span><br><span class="line">        <span class="built_in">print</span>(self.q_next)</span><br><span class="line">                </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">choose_action</span>(<span class="params">self, observation</span>):</span><br><span class="line">        <span class="comment">#根据observation（state）选行为</span></span><br><span class="line">        <span class="comment">#使用eval network选出state下的行为估计</span></span><br><span class="line">        <span class="comment">#将observation的shape变为(1, size_of_observation)，行向量变为列向量才能与NN维度统一</span></span><br><span class="line">        observation = observation[np.newaxis, :]</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span> np.random.uniform() &lt; self.epsilon:</span><br><span class="line">            action_value = self.sess.run(self.q_eval, feed_dict=&#123;self.s:observation&#125;)</span><br><span class="line">            action = np.argmax(action_value)</span><br><span class="line">            </span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            action = np.random.randint(<span class="number">0</span>, self.n_actions)   <span class="comment">#随机选择</span></span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> action</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">learn</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">if</span> self.learn_step_counter % self.replace_target_iter ==<span class="number">0</span>:</span><br><span class="line">            self.sess.run(self.replace_target_op)</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&#x27;\ntarget_params_replaced\n&#x27;</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="comment">#从memory中随机抽取batch_size这么多记忆</span></span><br><span class="line">        <span class="keyword">if</span> self.memory_counter &gt; self.memory_size:   <span class="comment">#说明记忆库已经存满，可以从记忆库任意位置收取</span></span><br><span class="line">            sample_index = np.random.choice(self.memory_size, size=self.batch_size)</span><br><span class="line">        <span class="keyword">else</span>:   <span class="comment">#记忆库还没有存满，从现有的存储记忆提取</span></span><br><span class="line">            sample_index = np.random.choice(self.memory_counter, size=self.batch_size)</span><br><span class="line">        </span><br><span class="line">        batch_memory= self.memory[sample_index, :]</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 获取q_next即q现实(target_net产生的q)和q_eval(eval_net产生的q)</span></span><br><span class="line">        <span class="comment">#q_next和q_eval都是一个向量，包含了对应状态下所有动作的q值</span></span><br><span class="line">        <span class="comment">#实际上feature为状态的维度，batch_memory[:, -self.n_features:]为s_,即状态s采取动作action后的状态s_, batch_memory[:, :self.n_features]为s</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 获取q_next即q现实(target_net产生的q)和q_eval(eval_net产生的q)</span></span><br><span class="line">        <span class="comment">#q_next和q_eval都是一个向量，包含了对应状态下所有动作的q值</span></span><br><span class="line">        <span class="comment">#实际上feature为状态的维度，batch_memory[:, -self.n_features:]为s_,即状态s采取动作action后的状态s_, batch_memory[:, :self.n_features]为s</span></span><br><span class="line">        <span class="comment">#q_next, q_eval的维度为[None,n_actions]</span></span><br><span class="line">        q_next, q_eval = self.sess.run([self.q_next, self.q_eval], feed_dict=&#123;self.s_: batch_memory[:, -self.n_features:],self.s: batch_memory[:, :self.n_features]&#125;)</span><br><span class="line">        </span><br><span class="line">        <span class="comment">#用t+1的状态带入eval network先选出动作</span></span><br><span class="line">        action_value = self.sess.run(self.q_eval, feed_dict=&#123;self.s: batch_memory[:, -self.n_features:]&#125;)</span><br><span class="line">        <span class="comment">#维度为[batch_size, 1]</span></span><br><span class="line">        action = np.argmax(action_value, axis=<span class="number">1</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="comment">#下面这几步十分重要. q_next, q_eval 包含所有 action 的值, 而我们需要的只是已经选择好的 action 的值, 其他的并不需要.所以我们将其他的 action 值全变成 0, 将用到的 action 误差值 反向传递回去, 作为更新凭据.</span></span><br><span class="line">        <span class="comment">#这是我们最终要达到的样子, 比如 q_target - q_eval = [1, 0, 0] - [-1, 0, 0] = [2, 0, 0]</span></span><br><span class="line">        <span class="comment"># q_eval = [-1, 0, 0] 表示这一个记忆中有我选用过 action 0, 而action0带来的 Q(s, a0)=-1,而其他的 Q(s, a1)=Q(s, a2)=0</span></span><br><span class="line">        <span class="comment"># q_target = [1, 0, 0] 表示这个记忆中的 r+gamma*maxQ(s_) = 1, 而且不管在 s_ 上我们取了哪个 action</span></span><br><span class="line">        <span class="comment"># 我们都需要对应上 q_eval 中的 action 位置, 所以就将 q_target的1放在了 action0的位置.</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 下面也是为了达到上面说的目的, 不过为了更方面让程序运算, 达到目的的过程有点不同.# 是将 q_eval 全部赋值给 q_target, 这时 q_target-q_eval 全为 0,</span></span><br><span class="line">        <span class="comment"># 不过 我们再根据 batch_memory 当中的 action 这个 column 来给 q_target 中的对应的 memory-action 位置来修改赋值.</span></span><br><span class="line">        <span class="comment"># 使新的赋值为 reward + gamma * maxQ(s_), 这样 q_target-q_eval 就可以变成我们所需的样子.</span></span><br><span class="line">        q_target = q_eval.copy()</span><br><span class="line">        <span class="comment">#每个样本下标</span></span><br><span class="line">        batch_index = np.arange(self.batch_size, dtype=np.int32)</span><br><span class="line">        <span class="comment">#记录每个样本执行的动作</span></span><br><span class="line">        eval_act_index = batch_memory[:, self.n_features].astype(<span class="built_in">int</span>)</span><br><span class="line">        <span class="comment">#记录每个样本动作的奖励</span></span><br><span class="line">        reward = batch_memory[:, self.n_features + <span class="number">1</span>]</span><br><span class="line">        <span class="comment">#生成每个样本中q值对应动作的更新，即生成的q现实， </span></span><br><span class="line">        q_target[batch_index, eval_act_index]=reward+self.gamma * q_next[batch_index, action] </span><br><span class="line">        </span><br><span class="line">        <span class="comment">#假如在这个 batch 中, 我们有2个提取的记忆, 根据每个记忆可以生产3个 action 的值:</span></span><br><span class="line">        <span class="comment">#q_eval =[[1, 2, 3],[4, 5, 6]]， 另q_target = q_eval.copy()</span></span><br><span class="line">        <span class="comment">#然后根据 memory 当中的具体 action 位置来修改 q_target 对应 action 上的值:</span></span><br><span class="line">        <span class="comment">#比如在:记忆 0 的 q_target 计算值是 -1, 而且我用了 action 0;忆1的 q_target 计算值是-2, 而且我用了 action 2:</span></span><br><span class="line">        <span class="comment">#q_target =[[-1, 2, 3],[4, 5, -2]]</span></span><br><span class="line">        <span class="comment">#所以 (q_target - q_eval) 就变成了:[[(-1)-(1), 0, 0],[0, 0, (-2)-(6)]]</span></span><br><span class="line">        <span class="comment">#最后我们将这个 (q_target - q_eval) 当成误差, 反向传递会神经网络</span></span><br><span class="line">        <span class="comment">#所有为 0 的 action 值是当时没有选择的 action, 之前有选择的 action 才有不为0的值.</span></span><br><span class="line">        <span class="comment">#我们只反向传递之前选择的 action 的值,</span></span><br><span class="line">        _, self.cost = self.sess.run([self._train_op, self.loss],feed_dict=&#123;self.s: batch_memory[:, :self.n_features],self.q_target: q_target&#125;)</span><br><span class="line">        </span><br><span class="line">        self.cost_his.append(self.cost) <span class="comment"># 记录 cost 误差</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment">#每调用一次learn，降低一次epsilon，即行为随机性</span></span><br><span class="line">        self.epsilon = self.epsilon + self.epsilon_increment <span class="keyword">if</span> self.epsilon &lt; self.epsilon_max <span class="keyword">else</span> self.epsilon_max</span><br><span class="line">        </span><br><span class="line">        self.learn_step_counter += <span class="number">1</span></span><br><span class="line">        </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">plot_cost</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">        plt.plot(np.arange(<span class="built_in">len</span>(self.cost_his)), self.cost_his)</span><br><span class="line">        plt.ylabel(<span class="string">&#x27;Cost&#x27;</span>)</span><br><span class="line">        plt.xlabel(<span class="string">&#x27;training steps&#x27;</span>)</span><br><span class="line">        plt.show()</span><br></pre></td></tr></table></figure>



<p>Double-DQN的agent编写与DQN几乎一样只是在求q估计的时候先用eval network求出价值最大的动作，再讲这个动作带入target network。<br>之前在DQN中使用的公式为</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">q_target[batch_index, eval_act_index]=reward+self.gamma * np.<span class="built_in">max</span>(q_next, axis=<span class="number">1</span>)</span><br><span class="line"><span class="comment">#改为：</span></span><br><span class="line"><span class="comment">#用t+1的状态带入eval network先选出动作</span></span><br><span class="line">action_value = self.sess.run(self.q_eval, feed_dict=&#123;self.s: batch_memory[:, -self.n_features:]&#125;)</span><br><span class="line"><span class="comment">#维度为[batch_size, 1]</span></span><br><span class="line">action = np.argmax(action_value, axis=<span class="number">1</span>)</span><br><span class="line">q_target[batch_index, eval_act_index]=reward+self.gamma * q_next[batch_index, action]</span><br></pre></td></tr></table></figure>



<h4 id="Duling-DQN"><a href="#Duling-DQN" class="headerlink" title="Duling DQN"></a>Duling DQN</h4><p><img src="https://pic-20201016.oss-cn-qingdao.aliyuncs.com/img/image-20210316204901396.png" alt="image-20210316204901396"></p>
<p><img src="https://pic-20201016.oss-cn-qingdao.aliyuncs.com/img/image-20210316204930844.png" alt="image-20210316204930844"></p>
<p><img src="https://pic-20201016.oss-cn-qingdao.aliyuncs.com/img/image-20210316205031940.png" alt="image-20210316205031940"></p>
<p>Dueling DQN与DQN的网络结构不同，其他过程相似。着重是更改原来DQN Agent的build_net()方法。<br>之前构建的方式是通过一个隐藏层直接获得q值</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> tf.variable_scope(<span class="string">&#x27;l1&#x27;</span>):</span><br><span class="line">                w2 = tf.get_variable(<span class="string">&#x27;w2&#x27;</span>, [n_l1, self.n_actions], initializer=w_initializer, collections=c_names)</span><br><span class="line">                b2 = tf.get_variable(<span class="string">&#x27;b2&#x27;</span>, [<span class="number">1</span>, self.n_actions], initializer=b_initializer, collections=c_names)</span><br><span class="line">                <span class="comment">#求出q估计值，长度为n_actions的向量</span></span><br><span class="line">                self.q_eval = tf.matmul(l1, w2) + b2</span><br><span class="line">                </span><br><span class="line"><span class="comment">#现在将q_eval 拆分为状态价值v和动作价值a，其中动作价值a的均值为0（q_target需要做同样的改动）：</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#状态的价值</span></span><br><span class="line">            <span class="keyword">with</span> tf.variable_scope(<span class="string">&#x27;value&#x27;</span>):</span><br><span class="line">                w21 = tf.get_variable(<span class="string">&#x27;w21&#x27;</span>, [n_l1, <span class="number">1</span>], initializer=w_initializer, collections=c_names)</span><br><span class="line">                b21 = tf.get_variable(<span class="string">&#x27;b21&#x27;</span>, [<span class="number">1</span>], initializer=b_initializer, collections=c_names)</span><br><span class="line">                vs_out = tf.matmul(l1, w21) + b21</span><br><span class="line">            </span><br><span class="line">            <span class="comment">#动作的advantage</span></span><br><span class="line">            <span class="keyword">with</span> tf.variable_scope(<span class="string">&#x27;advantage&#x27;</span>):</span><br><span class="line">                w22 = tf.get_variable(<span class="string">&#x27;w22&#x27;</span>, [n_l1, self.n_actions], initializer=w_initializer, collections=c_names)</span><br><span class="line">                b22 = tf.get_variable(<span class="string">&#x27;b22&#x27;</span>, [<span class="number">1</span>, self.n_actions], initializer=b_initializer, collections=c_names)</span><br><span class="line">                aa = tf.matmul(l1, w22) + b22</span><br><span class="line">                <span class="comment">#为了不让A直接学成了Q, 我们减掉了A的均值，此时A的均值始终为0</span></span><br><span class="line">                aa_out = aa - tf.reduce_mean(aa, axis=<span class="number">1</span>, keep_dims=<span class="literal">True</span>)</span><br><span class="line">                </span><br><span class="line">            <span class="comment">#合并V和A, 求出q估计值</span></span><br><span class="line">            <span class="keyword">with</span> tf.variable_scope(<span class="string">&#x27;Q&#x27;</span>):</span><br><span class="line">                self.q_eval = vs_out + aa_out</span><br><span class="line">                </span><br><span class="line">               </span><br></pre></td></tr></table></figure>

<p>完整的dueling DQN的Agent的代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Dueling_DeepQNetwork</span>(<span class="title class_ inherited__">object</span>):</span><br><span class="line">    <span class="comment">#replace_target_iter为更新target network的步数，防止target network和eval network差别过大</span></span><br><span class="line">    <span class="comment">#memory_size为buffer储存记忆上线，方便使用以前记忆学习</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, n_actions, n_features,learning_rate=<span class="number">0.01</span>,reward_decay=<span class="number">0.9</span>,e_greedy=<span class="number">0.9</span>,replace_target_iter=<span class="number">300</span>,memory_size=<span class="number">500</span>,batch_size=<span class="number">32</span>,e_greedy_increment=<span class="literal">None</span>,output_graph=<span class="literal">False</span></span>):</span><br><span class="line">        self.n_actions = n_actions</span><br><span class="line">        self.n_features = n_features</span><br><span class="line">        self.lr = learning_rate</span><br><span class="line">        self.gamma = reward_decay</span><br><span class="line">        self.epsilon_max = e_greedy     <span class="comment"># epsilon后面奖励对前面的递减参数</span></span><br><span class="line">        self.replace_target_iter = replace_target_iter  <span class="comment"># 更换 target_net 的步数</span></span><br><span class="line">        self.memory_size = memory_size  <span class="comment"># 记忆上限</span></span><br><span class="line">        self.batch_size = batch_size    <span class="comment"># 每次更新时从 memory 里面取多少记忆出来</span></span><br><span class="line">        self.epsilon_increment = e_greedy_increment <span class="comment"># epsilon 的增量</span></span><br><span class="line">        <span class="comment">#epsilon = 0等于0时，后面的奖励创传不到前面，前面的状态就开启随机探索模式</span></span><br><span class="line">        self.epsilon = <span class="number">0</span> <span class="keyword">if</span> e_greedy_increment <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span> <span class="keyword">else</span> self.epsilon_max <span class="comment"># 是否开启探索模式, 并逐步减少探索次数</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 记录学习次数 (用于判断是否更换 target_net 参数)</span></span><br><span class="line">        self.learn_step_counter = <span class="number">0</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 初始化全 0 记忆 [s, a, r, s_]， 实际上feature为状态的维度，n_features*2分别记录s和s_，+2记录a和r</span></span><br><span class="line">        self.memory = np.zeros((self.memory_size, n_features*<span class="number">2</span>+<span class="number">2</span>))</span><br><span class="line">        </span><br><span class="line">        self._build_net()</span><br><span class="line">        </span><br><span class="line">        <span class="comment">#替换 target net 的参数</span></span><br><span class="line">        t_params = tf.get_collection(<span class="string">&#x27;target_net_params&#x27;</span>)  <span class="comment">#提取 target_net 的参数</span></span><br><span class="line">        e_params = tf.get_collection(<span class="string">&#x27;eval_net_params&#x27;</span>)   <span class="comment"># 提取  eval_net 的参数</span></span><br><span class="line">        <span class="comment">#将eval_network中每一个variable的值赋值给target network的对应变量</span></span><br><span class="line">        self.replace_target_op = [tf.assign(t, e) <span class="keyword">for</span> t, e <span class="keyword">in</span> <span class="built_in">zip</span>(t_params, e_params)] <span class="comment">#更新 target_net 参数</span></span><br><span class="line">        </span><br><span class="line">        </span><br><span class="line">        self.sess = tf.Session()</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span> output_graph:</span><br><span class="line">            tf.summary.FileWriter(<span class="string">&quot;logs/&quot;</span>, self.sess.graph)</span><br><span class="line">        </span><br><span class="line">        self.sess.run(tf.global_variables_initializer())</span><br><span class="line">        <span class="comment">#用于记录# 记录所有 cost 变化</span></span><br><span class="line">        self.cost_his = []</span><br><span class="line">    </span><br><span class="line">    <span class="comment">#李宏毅老师克重的relpay buffer，通过以往的记忆中不断训练</span></span><br><span class="line">    <span class="comment">#这是DQN变为off-policy的核心</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">store_transition</span>(<span class="params">self, s, a, r, s_</span>):</span><br><span class="line">        <span class="comment">#如果DeepQNetwork中定义了memory_counter，进行记忆存储</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> <span class="built_in">hasattr</span>(self, <span class="string">&#x27;memory_counter&#x27;</span>):</span><br><span class="line">            self.memory_counter = <span class="number">0</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment">#记录一条 [s, a, r, s_] 记录</span></span><br><span class="line">        transition = np.hstack((s, [a, r], s_))</span><br><span class="line">        </span><br><span class="line">        <span class="comment">#总 memory 大小是固定的, 如果超出总大小, 旧 memory 就被新 memory 替换</span></span><br><span class="line">        index = self.memory_counter % self.memory_size  <span class="comment">#类似hashmap赋值思想</span></span><br><span class="line">        self.memory[index, :] = transition  <span class="comment">#进行替换</span></span><br><span class="line">        </span><br><span class="line">        self.memory_counter += <span class="number">1</span></span><br><span class="line">        </span><br><span class="line">    <span class="comment">#建立神经网络</span></span><br><span class="line">    <span class="comment">#此处建立两个申请网络，一个为target network，用于得到q现实。一个为eval_network，用于得到q估计</span></span><br><span class="line">    <span class="comment">#target network和eval_network结构一样，target network用比较老的参数，eval_network为真正训练的神经网络</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_build_net</span>(<span class="params">self</span>):</span><br><span class="line">        tf.reset_default_graph()  <span class="comment">#清空计算图</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment">#创建eval神经网络,及时提升参数</span></span><br><span class="line">        self.s = tf.placeholder(tf.float32, [<span class="literal">None</span>, self.n_features], name=<span class="string">&#x27;s&#x27;</span>)  <span class="comment"># 用来接收 observation，即神经网络的输入</span></span><br><span class="line">        self.q_target = tf.placeholder(tf.float32, [<span class="literal">None</span>, self.n_actions], name=<span class="string">&#x27;Q_target&#x27;</span>) <span class="comment"># q_target的值, 这个之后会通过计算得到，神经网络的输出</span></span><br><span class="line">        <span class="comment">#eval_net域下的变量</span></span><br><span class="line">        <span class="keyword">with</span> tf.variable_scope(<span class="string">&#x27;eval_net&#x27;</span>):</span><br><span class="line">            <span class="comment">#c_names用于在一定步数之后更新target network</span></span><br><span class="line">            <span class="comment">#GLOBAL_VARIABLES作用是collection默认加入所有的Variable对象，用于共享</span></span><br><span class="line">            c_names = [<span class="string">&#x27;eval_net_params&#x27;</span>, tf.GraphKeys.GLOBAL_VARIABLES]</span><br><span class="line">            n_l1 = <span class="number">10</span>  <span class="comment">#n_l1为network隐藏层神经元的个数 </span></span><br><span class="line">            w_initializer = tf.random_normal_initializer(<span class="number">0.</span>,<span class="number">0.3</span>)  </span><br><span class="line">            b_initializer = tf.constant_initializer(<span class="number">0.1</span>)</span><br><span class="line">            </span><br><span class="line">            <span class="comment">#eval_network第一层全连接神经网络</span></span><br><span class="line">            <span class="keyword">with</span> tf.variable_scope(<span class="string">&#x27;l1&#x27;</span>):</span><br><span class="line">                w1 = tf.get_variable(<span class="string">&#x27;w1&#x27;</span>, [self.n_features, n_l1], initializer=w_initializer, collections=c_names)</span><br><span class="line">                b1 = tf.get_variable(<span class="string">&#x27;b1&#x27;</span>, [<span class="number">1</span>, n_l1], initializer=b_initializer, collections=c_names)</span><br><span class="line">                l1 = tf.nn.relu(tf.matmul(self.s, w1)+b1)</span><br><span class="line">            </span><br><span class="line">            <span class="comment">#状态的价值</span></span><br><span class="line">            <span class="keyword">with</span> tf.variable_scope(<span class="string">&#x27;value&#x27;</span>):</span><br><span class="line">                w21 = tf.get_variable(<span class="string">&#x27;w21&#x27;</span>, [n_l1, <span class="number">1</span>], initializer=w_initializer, collections=c_names)</span><br><span class="line">                b21 = tf.get_variable(<span class="string">&#x27;b21&#x27;</span>, [<span class="number">1</span>], initializer=b_initializer, collections=c_names)</span><br><span class="line">                vs_out = tf.matmul(l1, w21) + b21</span><br><span class="line">            </span><br><span class="line">            <span class="comment">#动作的advantage</span></span><br><span class="line">            <span class="keyword">with</span> tf.variable_scope(<span class="string">&#x27;advantage&#x27;</span>):</span><br><span class="line">                w22 = tf.get_variable(<span class="string">&#x27;w22&#x27;</span>, [n_l1, self.n_actions], initializer=w_initializer, collections=c_names)</span><br><span class="line">                b22 = tf.get_variable(<span class="string">&#x27;b22&#x27;</span>, [<span class="number">1</span>, self.n_actions], initializer=b_initializer, collections=c_names)</span><br><span class="line">                aa = tf.matmul(l1, w22) + b22</span><br><span class="line">                <span class="comment">#为了不让A直接学成了Q, 我们减掉了A的均值，此时A的均值始终为0</span></span><br><span class="line">                aa_out = aa - tf.reduce_mean(aa, axis=<span class="number">1</span>, keep_dims=<span class="literal">True</span>)</span><br><span class="line">                </span><br><span class="line">            <span class="comment">#合并V和A, 求出q估计值</span></span><br><span class="line">            <span class="keyword">with</span> tf.variable_scope(<span class="string">&#x27;Q&#x27;</span>):</span><br><span class="line">                self.q_eval = vs_out + aa_out</span><br><span class="line">            </span><br><span class="line">        <span class="keyword">with</span> tf.variable_scope(<span class="string">&#x27;loss&#x27;</span>): <span class="comment"># 求误差</span></span><br><span class="line">            <span class="comment">#使用平方误差</span></span><br><span class="line">            self.loss = tf.reduce_mean(tf.squared_difference(self.q_target, self.q_eval))</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">with</span> tf.variable_scope(<span class="string">&#x27;train&#x27;</span>):    <span class="comment"># 梯度下降</span></span><br><span class="line">            optimizer = tf.train.RMSPropOptimizer(self.lr)</span><br><span class="line">            self._train_op = optimizer.minimize(self.loss)</span><br><span class="line">        </span><br><span class="line">        <span class="comment">#创建target network，输入选择一个action后的状态s_,输出q_target</span></span><br><span class="line">        self.s_ = tf.placeholder(tf.float32, [<span class="literal">None</span>, self.n_features], name=<span class="string">&#x27;s_&#x27;</span>)    <span class="comment"># 接收下个 observation</span></span><br><span class="line">        <span class="keyword">with</span> tf.variable_scope(<span class="string">&#x27;target_net&#x27;</span>):</span><br><span class="line">            c_names = [<span class="string">&#x27;target_net_params&#x27;</span>, tf.GraphKeys.GLOBAL_VARIABLES]</span><br><span class="line">            </span><br><span class="line">            <span class="comment"># target_net 的第一层fc， collections 是在更新 target_net 参数时会用到</span></span><br><span class="line">            <span class="keyword">with</span> tf.variable_scope(<span class="string">&#x27;l1&#x27;</span>):</span><br><span class="line">                w1 = tf.get_variable(<span class="string">&#x27;w1&#x27;</span>, [self.n_features, n_l1], initializer=w_initializer, collections=c_names)</span><br><span class="line">                b1 = tf.get_variable(<span class="string">&#x27;b1&#x27;</span>, [<span class="number">1</span>, n_l1], initializer=b_initializer, collections=c_names)</span><br><span class="line">                l1 = tf.nn.relu(tf.matmul(self.s_, w1) + b1)</span><br><span class="line"></span><br><span class="line">            <span class="comment">#状态的价值</span></span><br><span class="line">            <span class="keyword">with</span> tf.variable_scope(<span class="string">&#x27;value&#x27;</span>):</span><br><span class="line">                w21 = tf.get_variable(<span class="string">&#x27;w21&#x27;</span>, [n_l1, <span class="number">1</span>], initializer=w_initializer, collections=c_names)</span><br><span class="line">                b21 = tf.get_variable(<span class="string">&#x27;b21&#x27;</span>, [<span class="number">1</span>], initializer=b_initializer, collections=c_names)</span><br><span class="line">                vs_out = tf.matmul(l1, w21) + b21</span><br><span class="line">            </span><br><span class="line">            <span class="comment">#动作的advantage</span></span><br><span class="line">            <span class="keyword">with</span> tf.variable_scope(<span class="string">&#x27;advantage&#x27;</span>):</span><br><span class="line">                w22 = tf.get_variable(<span class="string">&#x27;w22&#x27;</span>, [n_l1, self.n_actions], initializer=w_initializer, collections=c_names)</span><br><span class="line">                b22 = tf.get_variable(<span class="string">&#x27;b22&#x27;</span>, [<span class="number">1</span>, self.n_actions], initializer=b_initializer, collections=c_names)</span><br><span class="line">                aa = tf.matmul(l1, w22) + b22</span><br><span class="line">                <span class="comment">#为了不让A直接学成了Q, 我们减掉了A的均值，此时A的均值始终为0</span></span><br><span class="line">                aa_out = aa - tf.reduce_mean(aa, axis=<span class="number">1</span>, keep_dims=<span class="literal">True</span>)</span><br><span class="line">            </span><br><span class="line">            <span class="comment">#合并V和A, 求出q估计值</span></span><br><span class="line">            <span class="keyword">with</span> tf.variable_scope(<span class="string">&#x27;Q&#x27;</span>):</span><br><span class="line">                self.q_next = vs_out + aa_out</span><br><span class="line">            </span><br><span class="line">        <span class="built_in">print</span>(self.q_next)</span><br><span class="line">                </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">choose_action</span>(<span class="params">self, observation</span>):</span><br><span class="line">        <span class="comment">#根据observation（state）选行为</span></span><br><span class="line">        <span class="comment">#使用eval network选出state下的行为估计</span></span><br><span class="line">        <span class="comment">#将observation的shape变为(1, size_of_observation)，行向量变为列向量才能与NN维度统一</span></span><br><span class="line">        observation = observation[np.newaxis, :]</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span> np.random.uniform() &lt; self.epsilon:</span><br><span class="line">            action_value = self.sess.run(self.q_eval, feed_dict=&#123;self.s:observation&#125;)</span><br><span class="line">            action = np.argmax(action_value)</span><br><span class="line">            </span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            action = np.random.randint(<span class="number">0</span>, self.n_actions)   <span class="comment">#随机选择</span></span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> action</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">learn</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">if</span> self.learn_step_counter % self.replace_target_iter ==<span class="number">0</span>:</span><br><span class="line">            self.sess.run(self.replace_target_op)</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&#x27;\ntarget_params_replaced\n&#x27;</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="comment">#从memory中随机抽取batch_size这么多记忆</span></span><br><span class="line">        <span class="keyword">if</span> self.memory_counter &gt; self.memory_size:   <span class="comment">#说明记忆库已经存满，可以从记忆库任意位置收取</span></span><br><span class="line">            sample_index = np.random.choice(self.memory_size, size=self.batch_size)</span><br><span class="line">        <span class="keyword">else</span>:   <span class="comment">#记忆库还没有存满，从现有的存储记忆提取</span></span><br><span class="line">            sample_index = np.random.choice(self.memory_counter, size=self.batch_size)</span><br><span class="line">        </span><br><span class="line">        batch_memory= self.memory[sample_index, :]</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 获取q_next即q现实(target_net产生的q)和q_eval(eval_net产生的q)</span></span><br><span class="line">        <span class="comment">#q_next和q_eval都是一个向量，包含了对应状态下所有动作的q值</span></span><br><span class="line">        <span class="comment">#实际上feature为状态的维度，batch_memory[:, -self.n_features:]为s_,即状态s采取动作action后的状态s_, batch_memory[:, :self.n_features]为s</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 获取q_next即q现实(target_net产生的q)和q_eval(eval_net产生的q)</span></span><br><span class="line">        <span class="comment">#q_next和q_eval都是一个向量，包含了对应状态下所有动作的q值</span></span><br><span class="line">        <span class="comment">#实际上feature为状态的维度，batch_memory[:, -self.n_features:]为s_,即状态s采取动作action后的状态s_, batch_memory[:, :self.n_features]为s</span></span><br><span class="line">        <span class="comment">#q_next, q_eval的维度为[None,n_actions]</span></span><br><span class="line">        q_next, q_eval = self.sess.run([self.q_next, self.q_eval], feed_dict=&#123;self.s_: batch_memory[:, -self.n_features:],self.s: batch_memory[:, :self.n_features]&#125;)</span><br><span class="line">        </span><br><span class="line">        <span class="comment">#下面这几步十分重要. q_next, q_eval 包含所有 action 的值, 而我们需要的只是已经选择好的 action 的值, 其他的并不需要.所以我们将其他的 action 值全变成 0, 将用到的 action 误差值 反向传递回去, 作为更新凭据.</span></span><br><span class="line">        <span class="comment">#这是我们最终要达到的样子, 比如 q_target - q_eval = [1, 0, 0] - [-1, 0, 0] = [2, 0, 0]</span></span><br><span class="line">        <span class="comment"># q_eval = [-1, 0, 0] 表示这一个记忆中有我选用过 action 0, 而action0带来的 Q(s, a0)=-1,而其他的 Q(s, a1)=Q(s, a2)=0</span></span><br><span class="line">        <span class="comment"># q_target = [1, 0, 0] 表示这个记忆中的 r+gamma*maxQ(s_) = 1, 而且不管在 s_ 上我们取了哪个 action</span></span><br><span class="line">        <span class="comment"># 我们都需要对应上 q_eval 中的 action 位置, 所以就将 q_target的1放在了 action0的位置.</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 下面也是为了达到上面说的目的, 不过为了更方面让程序运算, 达到目的的过程有点不同.# 是将 q_eval 全部赋值给 q_target, 这时 q_target-q_eval 全为 0,</span></span><br><span class="line">        <span class="comment"># 不过 我们再根据 batch_memory 当中的 action 这个 column 来给 q_target 中的对应的 memory-action 位置来修改赋值.</span></span><br><span class="line">        <span class="comment"># 使新的赋值为 reward + gamma * maxQ(s_), 这样 q_target-q_eval 就可以变成我们所需的样子.</span></span><br><span class="line">        </span><br><span class="line">        q_target = q_eval.copy()</span><br><span class="line">        <span class="comment">#每个样本下标</span></span><br><span class="line">        batch_index = np.arange(self.batch_size, dtype=np.int32)</span><br><span class="line">        <span class="comment">#记录每个样本执行的动作</span></span><br><span class="line">        eval_act_index = batch_memory[:, self.n_features].astype(<span class="built_in">int</span>)</span><br><span class="line">        <span class="comment">#记录每个样本动作的奖励</span></span><br><span class="line">        reward = batch_memory[:, self.n_features + <span class="number">1</span>]</span><br><span class="line">        </span><br><span class="line">        <span class="comment">#生成每个样本中q值对应动作的更新，即生成的q现实，</span></span><br><span class="line">        q_target[batch_index, eval_act_index]=reward+self.gamma * np.<span class="built_in">max</span>(q_next, axis=<span class="number">1</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="comment">#假如在这个 batch 中, 我们有2个提取的记忆, 根据每个记忆可以生产3个 action 的值:</span></span><br><span class="line">        <span class="comment">#q_eval =[[1, 2, 3],[4, 5, 6]]， 另q_target = q_eval.copy()</span></span><br><span class="line">        <span class="comment">#然后根据 memory 当中的具体 action 位置来修改 q_target 对应 action 上的值:</span></span><br><span class="line">        <span class="comment">#比如在:记忆 0 的 q_target 计算值是 -1, 而且我用了 action 0;忆1的 q_target 计算值是-2, 而且我用了 action 2:</span></span><br><span class="line">        <span class="comment">#q_target =[[-1, 2, 3],[4, 5, -2]]</span></span><br><span class="line">        <span class="comment">#所以 (q_target - q_eval) 就变成了:[[(-1)-(1), 0, 0],[0, 0, (-2)-(6)]]</span></span><br><span class="line">        <span class="comment">#最后我们将这个 (q_target - q_eval) 当成误差, 反向传递会神经网络</span></span><br><span class="line">        <span class="comment">#所有为 0 的 action 值是当时没有选择的 action, 之前有选择的 action 才有不为0的值.</span></span><br><span class="line">        <span class="comment">#我们只反向传递之前选择的 action 的值,</span></span><br><span class="line">        _, self.cost = self.sess.run([self._train_op, self.loss],feed_dict=&#123;self.s: batch_memory[:, :self.n_features],self.q_target: q_target&#125;)</span><br><span class="line">        </span><br><span class="line">        self.cost_his.append(self.cost) <span class="comment"># 记录 cost 误差</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment">#每调用一次learn，降低一次epsilon，即行为随机性</span></span><br><span class="line">        self.epsilon = self.epsilon + self.epsilon_increment <span class="keyword">if</span> self.epsilon &lt; self.epsilon_max <span class="keyword">else</span> self.epsilon_max</span><br><span class="line">        </span><br><span class="line">        self.learn_step_counter += <span class="number">1</span></span><br><span class="line">        </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">plot_cost</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">        plt.plot(np.arange(<span class="built_in">len</span>(self.cost_his)), self.cost_his)</span><br><span class="line">        plt.ylabel(<span class="string">&#x27;Cost&#x27;</span>)</span><br><span class="line">        plt.xlabel(<span class="string">&#x27;training steps&#x27;</span>)</span><br><span class="line">        plt.show()</span><br></pre></td></tr></table></figure>



<h4 id="Policy-Gradient"><a href="#Policy-Gradient" class="headerlink" title="Policy Gradient"></a>Policy Gradient</h4><p>之前说的SARSA、Q-learning、DQN学习的都是在状态s下动作a的价值，属于value-based的方法。而Policy Gradient学习的是在状态s下每个动作a被选择的概率，属于policy-based的方法。</p>
<p>我们先说Policy Gradient的整体思想，之后将整体思想进行拆分，产生Policy Gradient每一步的流程。</p>
<p>Policy Gradient的网络要学习的是状态下动作输出的概率。按照常识来讲，可以获得越大的奖励的动作应该被选择的概率是越大的。需要注意的是这里所说的奖励并不是一个动作单步的奖励，而是当整个游戏结束时，这个动作整体所产生的价值，这个价值我们叫做advantage。因此我们的网络要学习的目标就是：按照每个动作的概率进行选择时，获得的奖励的期望值是最大的。</p>
<p><a target="_blank" rel="noopener" href="https://www.jianshu.com/p/428b640046aa">https://www.jianshu.com/p/428b640046aa</a></p>
<h5 id="策略梯度"><a href="#策略梯度" class="headerlink" title="策略梯度"></a>策略梯度</h5><p>在PG算法中，我们的Agent又被称为Actor，Actor对于一个特定的任务，都有自己的一个策略π，策略π通常用一个神经网络表示，其参数为θ。从一个特定的状态state出发，一直到任务的结束，被称为一个完整的eposide，在每一步，我们都能获得一个奖励r，一个完整的任务所获得的最终奖励被称为R。这样，一个有T个时刻的eposide，Actor不断与环境交互，形成如下的序列τ：</p>
<p><img src="https://pic-20201016.oss-cn-qingdao.aliyuncs.com/img/image-20210316210642889.png" alt="image-20210316210642889"></p>
<p><img src="https://pic-20201016.oss-cn-qingdao.aliyuncs.com/img/image-20210311213430270.png" alt="image-20210311213430270"></p>
<p>这样一个序列τ是不确定的，因为Actor在不同state下所采取的action可能是不同的，一个序列τ发生的概率为：</p>
<p><img src="https://pic-20201016.oss-cn-qingdao.aliyuncs.com/img/image-20210311213447597.png" alt="image-20210311213447597"></p>
<p>序列τ所获得的奖励为每个阶段所得到的奖励的和，称为R(τ)。因此，在Actor的策略为π的情况下，所能获得的期望奖励为：</p>
<p><img src="https://pic-20201016.oss-cn-qingdao.aliyuncs.com/img/image-20210311213514694.png" alt="image-20210311213514694"></p>
<p>而我们的期望是调整Actor的策略π，使得期望奖励最大化，于是我们有了策略梯度的方法，既然我们的期望函数已经有了，我们只要使用梯度提升的方法更新我们的网络参数θ（即更新策略π）就好了，所以问题的重点变为了求参数的梯度。梯度的求解过程如下：</p>
<p><img src="https://pic-20201016.oss-cn-qingdao.aliyuncs.com/img/image-20210316210915795.png" alt="image-20210316210915795"></p>
<p>首先利用log函数求导的特点进行转化，随后用N次采样的平均值来近似期望，最后，我们将pθ展开，将与θ无关的项去掉，即得到了最终的结果。</p>
<p><img src="https://pic-20201016.oss-cn-qingdao.aliyuncs.com/img/image-20210311213315228.png" alt="image-20210311213315228"></p>
<p><img src="https://pic-20201016.oss-cn-qingdao.aliyuncs.com/img/image-20210311213339458.png" alt="image-20210311213339458"></p>
<p><img src="https://pic-20201016.oss-cn-qingdao.aliyuncs.com/img/image-20210316211600007.png" alt="image-20210316211600007"></p>
<p><img src="https://pic-20201016.oss-cn-qingdao.aliyuncs.com/img/image-20210316211642310.png" alt="image-20210316211642310"></p>
<p><img src="https://pic-20201016.oss-cn-qingdao.aliyuncs.com/img/image-20210316211734692.png" alt="image-20210316211734692"></p>
<p><img src="https://pic-20201016.oss-cn-qingdao.aliyuncs.com/img/image-20210316211747501.png" alt="image-20210316211747501"></p>
<p><img src="https://pic-20201016.oss-cn-qingdao.aliyuncs.com/img/image-20210316211839980.png" alt="image-20210316211821933"></p>
<p>policy gradient 的Agent的实现</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> maze_env_drl <span class="keyword">import</span> Maze</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">PolicyGradient</span>(<span class="title class_ inherited__">object</span>):</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, n_actions, n_features, learning_rate=<span class="number">0.01</span>, reward_decay=<span class="number">0.95</span>, output_graph=<span class="literal">False</span></span>):</span><br><span class="line">        self.n_actions = n_actions</span><br><span class="line">        self.n_features = n_features</span><br><span class="line">        self.lr = learning_rate     <span class="comment"># 学习率</span></span><br><span class="line">        self.gamma = reward_decay   <span class="comment"># reward 递减率</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment">#因为需要模拟整个回合才能得到最终奖励，才能进行网络学习，所以需要将达到终点点整个序列的状态、动作、奖励记录下来</span></span><br><span class="line">        self.ep_obs, self.ep_as, self.ep_rs = [], [], []    <span class="comment">#这是我们存储 回合信息的 list</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment">#下面与之前是一样的</span></span><br><span class="line">        self._build_net()  <span class="comment">#建立网络</span></span><br><span class="line">        self.sess = tf.Session()</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span> output_graph:</span><br><span class="line">            tf.summary.FileWriter(<span class="string">&quot;logs/&quot;</span>, self.sess.graph)</span><br><span class="line">        </span><br><span class="line">        self.sess.run(tf.global_variables_initializer())</span><br><span class="line">        </span><br><span class="line">    <span class="comment">#与此前不同之前储存记忆利用的是replay buffer机制。存储内容分可能来自不同执行序列和不同参数的神经网络</span></span><br><span class="line">    <span class="comment">#此处只是为了储存达到终点前一个系列的动作</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">store_transition</span>(<span class="params">self, s, a, r</span>):</span><br><span class="line">        self.ep_obs.append(s)</span><br><span class="line">        self.ep_as.append(a)</span><br><span class="line">        self.ep_rs.append(r)</span><br><span class="line">    </span><br><span class="line">    <span class="comment">#构建网络，因为直接走到终点，奖励是可以观察的，不再需要target_work去求st+1的最大价值</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_build_net</span>(<span class="params">self</span>):</span><br><span class="line">        tf.reset_default_graph()  <span class="comment">#清空计算图</span></span><br><span class="line">        <span class="keyword">with</span> tf.name_scope(<span class="string">&#x27;input&#x27;</span>):</span><br><span class="line">            self.tf_obs = tf.placeholder(tf.float32, [<span class="literal">None</span>, self.n_features], name=<span class="string">&quot;observations&quot;</span>)  <span class="comment">#接收observation</span></span><br><span class="line">            <span class="comment">#标签维度为[batch_size, 1]。标签是具体动作，不是概率</span></span><br><span class="line">            self.tf_acts = tf.placeholder(tf.int32, [<span class="literal">None</span>, ], name=<span class="string">&quot;actions_num&quot;</span>)   <span class="comment"># 接收我们在这个回合中选过的actions</span></span><br><span class="line">            <span class="comment">#接收每个state-action所对应的value(通过reward计算),注意此处不是单步的奖励，而是整个一些列动作的奖励,vt=本reward + 衰减的未来reward</span></span><br><span class="line">            self.tf_vt = tf.placeholder(tf.float32, [<span class="literal">None</span>, ], name=<span class="string">&quot;actions_value&quot;</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="comment">#权重初始化方式</span></span><br><span class="line">        w_initializer = tf.random_normal_initializer(<span class="number">0.</span>,<span class="number">0.3</span>)  </span><br><span class="line">        b_initializer = tf.constant_initializer(<span class="number">0.1</span>)</span><br><span class="line">        n_l1 = <span class="number">10</span>  <span class="comment">#n_l1为network隐藏层神经元的个数 </span></span><br><span class="line">        <span class="keyword">with</span> tf.variable_scope(<span class="string">&quot;lay1&quot;</span>):</span><br><span class="line">            w1 = tf.get_variable(<span class="string">&#x27;w1&#x27;</span>, [self.n_features, n_l1], initializer=w_initializer)</span><br><span class="line">            b1 = tf.get_variable(<span class="string">&#x27;b1&#x27;</span>, [<span class="number">1</span>, n_l1], initializer=b_initializer)</span><br><span class="line">            l1 = tf.nn.tanh(tf.matmul(self.tf_obs, w1)+b1)</span><br><span class="line">        <span class="keyword">with</span> tf.variable_scope(<span class="string">&quot;lay2&quot;</span>):</span><br><span class="line">            w2 = tf.get_variable(<span class="string">&#x27;w2&#x27;</span>, [n_l1, self.n_actions], initializer=w_initializer)</span><br><span class="line">            b2 = tf.get_variable(<span class="string">&#x27;b2&#x27;</span>, [<span class="number">1</span>, self.n_actions], initializer=b_initializer)</span><br><span class="line">            output = tf.matmul(l1, w2)+b2</span><br><span class="line">        </span><br><span class="line">        <span class="comment">#ploicy gradient求解在一个状态下每个动作的概率，因此使用softmax出动作概率</span></span><br><span class="line">        self.all_act_prob = tf.nn.softmax(output)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">with</span> tf.name_scope(<span class="string">&#x27;loss&#x27;</span>):</span><br><span class="line">            <span class="comment">#交叉熵作为损失函数,训练样本中选中的action即为我们的label,因为还没求出最终损失,此处用reduce_sum</span></span><br><span class="line">            neg_log_prob = -tf.reduce_sum(tf.log(self.all_act_prob)*tf.one_hot(self.tf_acts, self.n_actions), axis=<span class="number">1</span>)</span><br><span class="line">            <span class="comment">#可是视为当前这个局部损失*对应权重，可以看到state-action的value越大，权重越大</span></span><br><span class="line">            <span class="comment">#可以认为state-action的value越大，就越会顺着这个梯度下降，这个动作就越可信</span></span><br><span class="line">            <span class="comment">#若不加基准线b为如下方式：</span></span><br><span class="line">            <span class="comment">#loss = tf.reduce_mean(neg_log_prob * (self.tf_vt))</span></span><br><span class="line">            </span><br><span class="line">            <span class="comment">#加入基准线b的损失</span></span><br><span class="line">            loss = tf.reduce_mean(neg_log_prob * (self.tf_vt-tf.reduce_mean(self.tf_vt)))</span><br><span class="line">            </span><br><span class="line">        <span class="keyword">with</span> tf.name_scope(<span class="string">&#x27;train&#x27;</span>):</span><br><span class="line">            self.train_op = tf.train.AdamOptimizer(self.lr).minimize(loss)</span><br><span class="line">    </span><br><span class="line">    <span class="comment">#之前DQN根据最大动作价值选择动作，ploicy gradient根据网络输出的动作概率，依照概率选择动作</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">choose_action</span>(<span class="params">self, observation</span>):</span><br><span class="line">        <span class="comment"># 所有action的概率</span></span><br><span class="line">        prob_weights = self.sess.run(self.all_act_prob, feed_dict=&#123;self.tf_obs: observation[np.newaxis, :]&#125;)</span><br><span class="line">        <span class="comment">#按照概率选择动作</span></span><br><span class="line">        action = np.random.choice(<span class="built_in">range</span>(prob_weights.shape[<span class="number">1</span>]), p=prob_weights.ravel())</span><br><span class="line">        <span class="keyword">return</span> action</span><br><span class="line">    </span><br><span class="line">    <span class="comment">#用于计算回合的state-action value</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_discount_and_norm_rewards</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="comment">#先计算单步的奖励，在逐步将后面的奖励逐步衰减加到前面</span></span><br><span class="line">        discounted_ep_rs = np.zeros_like(self.ep_rs)</span><br><span class="line">        running_add = <span class="number">0</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment">#将后面的奖励逐步衰减加到前面</span></span><br><span class="line">        <span class="keyword">for</span> t <span class="keyword">in</span> <span class="built_in">reversed</span>(<span class="built_in">range</span>(<span class="built_in">len</span>(self.ep_rs))):</span><br><span class="line">            running_add = running_add * self.gamma + self.ep_rs[t]</span><br><span class="line">            discounted_ep_rs[t] = running_add</span><br><span class="line">            </span><br><span class="line">        <span class="comment">#防止每个回合计算出的奖励量纲不同，进行正态标准化</span></span><br><span class="line">        discounted_ep_rs = np.array(discounted_ep_rs,dtype=np.<span class="built_in">float</span>) </span><br><span class="line">        discounted_ep_rs -= np.mean(discounted_ep_rs)</span><br><span class="line">        discounted_ep_rs /= np.std(discounted_ep_rs)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> discounted_ep_rs</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">learn</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="comment">#因为训练时传入的不是动作单步奖励，执行完动作后的整体奖励，因此先计算执行完动作后的整体奖励</span></span><br><span class="line">        discounted_ep_rs_norm = self._discount_and_norm_rewards()</span><br><span class="line">        <span class="comment">#np.vstack(self.ep_obs)的shape=[None, n_obs]，np.array(self.ep_as)的shape=[None,]，discounted_ep_rs_norm的shape=[None,]</span></span><br><span class="line">        <span class="comment">#每次的训练样本相当于一和完整流程状态和对应的动作以及对应奖励。</span></span><br><span class="line">        self.sess.run(self.train_op, feed_dict=&#123;self.tf_obs: np.vstack(self.ep_obs),self.tf_acts: np.array(self.ep_as),self.tf_vt: discounted_ep_rs_norm&#125;)</span><br><span class="line">        <span class="comment">#次回合动作已经训练完，清空记忆。下次训练需要重新产生训练样本</span></span><br><span class="line">        self.ep_obs, self.ep_as, self.ep_rs = [], [], []</span><br><span class="line">        </span><br><span class="line">        <span class="comment">#返回这一回合的state-action value</span></span><br><span class="line">        <span class="keyword">return</span> discounted_ep_rs_norm  </span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">run_maze</span>(<span class="params">RENDER</span>):</span><br><span class="line">    <span class="keyword">for</span> episode <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">3000</span>):</span><br><span class="line">        <span class="comment"># 初始化环境</span></span><br><span class="line">        observation = env.reset()</span><br><span class="line">        <span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">            <span class="keyword">if</span> RENDER:</span><br><span class="line">                <span class="comment"># 刷新环境</span></span><br><span class="line">                env.render()</span><br><span class="line">            <span class="comment"># DQN 根据观测值选择行为</span></span><br><span class="line">            action = RL.choose_action(observation)</span><br><span class="line">            <span class="comment"># 环境根据行为给出下一个state, reward,是否终止</span></span><br><span class="line">            observation_, reward, done = env.step(action)</span><br><span class="line">            <span class="comment">#存储记忆</span></span><br><span class="line">            RL.store_transition(observation, action, reward)</span><br><span class="line">            <span class="comment">#知道获得最终奖励才能进行训练，这是MC的方法</span></span><br><span class="line">            <span class="keyword">if</span> done:</span><br><span class="line">                ep_rs_sum=<span class="built_in">sum</span>(RL.ep_rs)</span><br><span class="line">                </span><br><span class="line">                <span class="keyword">if</span> <span class="string">&#x27;running_reward&#x27;</span> <span class="keyword">not</span> <span class="keyword">in</span> <span class="built_in">locals</span>():</span><br><span class="line">                    running_reward = ep_rs_sum</span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    running_reward = running_reward * <span class="number">0.99</span> + ep_rs_sum * <span class="number">0.01</span></span><br><span class="line">                </span><br><span class="line">                <span class="keyword">if</span> running_reward&gt; DISPLAY_REWARD_THRESHOLD:</span><br><span class="line">                    RENDER=<span class="literal">True</span></span><br><span class="line">                </span><br><span class="line">                <span class="built_in">print</span>(<span class="string">&quot;episode:&quot;</span>, episode, <span class="string">&quot;  reward:&quot;</span>, <span class="built_in">int</span>(running_reward))</span><br><span class="line">                </span><br><span class="line">                vt = RL.learn()</span><br><span class="line">                </span><br><span class="line">                <span class="keyword">if</span> episode==<span class="number">0</span>:</span><br><span class="line">                    plt.plot(vt)    <span class="comment"># plot这个回合的vt</span></span><br><span class="line">                    plt.xlabel(<span class="string">&#x27;episode steps&#x27;</span>)</span><br><span class="line">                    plt.ylabel(<span class="string">&#x27;normalized state-action value&#x27;</span>)</span><br><span class="line">                    plt.show()</span><br><span class="line">                <span class="keyword">break</span>   <span class="comment">#此次模拟完毕</span></span><br><span class="line">                </span><br><span class="line">            observation = observation_  <span class="comment">#还没有获取奖励，去要继续执行模拟</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__==<span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    RENDER=<span class="literal">False</span></span><br><span class="line">    DISPLAY_REWARD_THRESHOLD=<span class="number">300</span></span><br><span class="line">    env = Maze() </span><br><span class="line">    </span><br><span class="line">    RL = PolicyGradient(n_actions=env.n_actions, n_features=env.n_features, learning_rate=<span class="number">0.02</span>, reward_decay=<span class="number">0.99</span>, output_graph=<span class="literal">True</span>)</span><br><span class="line">    run_maze(RENDER)</span><br></pre></td></tr></table></figure>

<p><img src="https://pic-20201016.oss-cn-qingdao.aliyuncs.com/img/image-20210317112639995.png" alt="image-20210317112639995"></p>
<p>可以看出policy gradient是基于回合更新的，因此我们需要需要很大的耐心和环境产生交互样本，进行回合训练，并且由于每个回合是不稳定的，因此我们需要的大量的样本</p>
<p><img src="https://pic-20201016.oss-cn-qingdao.aliyuncs.com/img/image-20210317155054551.png" alt="image-20210317155054551"></p>
<p>Q-learning学习的是每个动作的价值，要求动作必须是离散的。</p>
<p>policy gradient和Q-learning都有各自的优缺点。我们可以将两者整合起来，即记忆使用off-policy的学习，也可以使用连续的动作。</p>
<p>下面两段话摘自莫烦python的强化学习教程：Actor-Critic 的 Actor 的前生是 Policy Gradients, 这能让它毫不费力地在连续动作中选取合适的动作, 而 Q-learning 做这件事会瘫痪. 那为什么不直接用 Policy Gradients 呢? 原来 Actor Critic 中的 Critic 的前生是 Q-learning 或者其他的 以值为基础的学习法 , 能进行单步更新, 而传统的 Policy Gradients 则是回合更新, 这降低了学习效率。</p>
<p><img src="https://pic-20201016.oss-cn-qingdao.aliyuncs.com/img/image-20210317155324880.png" alt="image-20210317155324880"></p>
<p>Actor 和 Critic, 他们都能用不同的神经网络来代替 . 在 Policy Gradients 的影片中提到过, 现实中的奖惩会左右 Actor 的更新情况. Policy Gradients 也是靠着这个来获取适宜的更新. 那么何时会有奖惩这种信息能不能被学习呢? 这看起来不就是 以值为基础的强化学习方法做过的事吗. 那我们就拿一个 Critic 去学习这些奖惩机制, 学习完了以后. 由 Actor 来指手画脚, 由 Critic 来告诉 Actor 你的那些指手画脚哪些指得好, 哪些指得差, Critic 通过学习环境和奖励之间的关系, 能看到现在所处状态的潜在奖励, 所以用它来指点 Actor 便能使 Actor 每一步都在更新, 如果使用单纯的 Policy Gradients, Actor 只能等到回合结束才能开始更新。</p>
<p>以前我们用过回合的奖励来进行policy gradient的更新，Actor-Critic将回合奖励替换成动作的价值，来对网络进行学习，自然就将Q-learning结合了起来：</p>
<p><img src="https://pic-20201016.oss-cn-qingdao.aliyuncs.com/img/image-20210317155528931.png" alt="image-20210317155528931"></p>
<p><img src="https://pic-20201016.oss-cn-qingdao.aliyuncs.com/img/image-20210317155542474.png" alt="image-20210317155542474"></p>
<p>在PG策略中，如果我们用Q函数来代替R，同时我们创建一个Critic网络来计算Q函数值，那么我们就得到了Actor-Critic方法。Actor参数的梯度变为：</p>
<p><img src="https://pic-20201016.oss-cn-qingdao.aliyuncs.com/img/image-20210311221748445.png" alt="image-20210311221748445"></p>
<p>此时的Critic根据估计的Q值和实际Q值的<strong>平方误差</strong>进行更新，对Critic来说，其loss为：</p>
<p><img src="https://pic-20201016.oss-cn-qingdao.aliyuncs.com/img/image-20210311221814333.png" alt="image-20210311221814333"></p>
<p>AC代码的实现地址为：<a target="_blank" rel="noopener" href="https://github.com/princewen/tensorflow_practice/tree/master/RL/Basic-AC-Demo">https://github.com/princewen/tensorflow_practice/tree/master/RL/Basic-AC-Demo</a></p>
<h1 id="Advantage-Actor-Critic-A2C"><a href="#Advantage-Actor-Critic-A2C" class="headerlink" title="Advantage Actor-Critic(A2C)"></a>Advantage Actor-Critic(A2C)</h1><p><img src="https://pic-20201016.oss-cn-qingdao.aliyuncs.com/img/image-20210317155623826.png" alt="image-20210317155623826"></p>
<p><img src="https://pic-20201016.oss-cn-qingdao.aliyuncs.com/img/image-20210317155634647.png" alt="image-20210317155634647"></p>
<p><img src="https://pic-20201016.oss-cn-qingdao.aliyuncs.com/img/image-20210317155755636.png" alt="image-20210317155755636"></p>
<p><img src="https://pic-20201016.oss-cn-qingdao.aliyuncs.com/img/image-20210317155831237.png" alt="image-20210317155831237"></p>
<p><img src="https://pic-20201016.oss-cn-qingdao.aliyuncs.com/img/image-20210317155955489.png" alt="image-20210317155955489"></p>
<p><img src="https://pic-20201016.oss-cn-qingdao.aliyuncs.com/img/image-20210317160425425.png" alt="image-20210317160425425"></p>
<p><img src="https://pic-20201016.oss-cn-qingdao.aliyuncs.com/img/image-20210317160942244.png" alt="image-20210317160942244"></p>
<p><img src="https://pic-20201016.oss-cn-qingdao.aliyuncs.com/img/image-20210317162607727.png" alt="image-20210317162607727"></p>
<p>本文基于如下架构进行Advantage Actor-Critic的Agent实现。其中Actor和Critic的第一层权重共享：</p>
<h4 id="Q-learning"><a href="#Q-learning" class="headerlink" title="Q-learning"></a>Q-learning</h4><p>Q-learning是一种value_based的方法。</p>
<p><img src="https://pic-20201016.oss-cn-qingdao.aliyuncs.com/img/image-20210312203323472.png" alt="image-20210312203323472"></p>
</div><div class="tags"><a href="/peaky/tags/%E5%AD%A6%E4%B9%A0/"><i class="fa fa-tag"></i>学习</a><a href="/peaky/tags/RL/"><i class="fa fa-tag"></i>RL</a></div><div class="post-nav"><a class="pre" href="/peaky/2021/05/05/a54293f2.html"></a><a class="next" href="/peaky/2021/03/01/514027fa.html"></a></div><div id="vcomment"></div><script src="//cdn1.lncld.net/static/js/3.0.4/av-min.js"></script><script src="//unpkg.com/valine@latest/dist/Valine.min.js"></script><script>var notify = 'false' == 'true' ? true : false;
var verify = 'false' == 'true' ? true : false;
var GUEST_INFO = ['nick','mail','link'];
var guest_info = 'nick,mail,link'.split(',').filter(function(item){
  return GUEST_INFO.indexOf(item) > -1
});
guest_info = guest_info.length == 0 ? GUEST_INFO :guest_info;
window.valine = new Valine({
  el:'#vcomment',
  notify:notify,
  verify:verify,
  appId:'c2tSICknvMNwQOvqeqh8wYoi-9Nh9j0Va',
  appKey:'geSzJMrX46ogsQR590PTiGPF',
  placeholder:'Just so so',
  avatar:'mm',
  guest_info:guest_info,
  pageSize:'10'
})
</script></div></div></div><div class="pure-u-1-4 hidden_mid_and_down"><div id="sidebar"><div class="widget"><div class="search-form"><input id="local-search-input" placeholder="Search" type="text" name="q" results="0"/><div id="local-search-result"></div></div></div><div class="widget"><div class="widget-title"><i class="fa fa-folder-o"> Kategorien</i></div><ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/peaky/categories/Linux/">Linux</a></li><li class="category-list-item"><a class="category-list-link" href="/peaky/categories/MIUI/">MIUI</a></li><li class="category-list-item"><a class="category-list-link" href="/peaky/categories/MarkDown/">MarkDown</a></li><li class="category-list-item"><a class="category-list-link" href="/peaky/categories/RL/">RL</a></li><li class="category-list-item"><a class="category-list-link" href="/peaky/categories/python/">python</a></li><li class="category-list-item"><a class="category-list-link" href="/peaky/categories/%E5%AD%A6%E4%B9%A0/">学习</a></li><li class="category-list-item"><a class="category-list-link" href="/peaky/categories/%E5%AF%BC%E8%88%AA/">导航</a></li><li class="category-list-item"><a class="category-list-link" href="/peaky/categories/%E6%95%99%E7%A8%8B/">教程</a></li><li class="category-list-item"><a class="category-list-link" href="/peaky/categories/%E7%94%B5%E8%84%91/">电脑</a></li><li class="category-list-item"><a class="category-list-link" href="/peaky/categories/%E7%BC%96%E7%A8%8B/">编程</a></li><li class="category-list-item"><a class="category-list-link" href="/peaky/categories/%E8%BD%AF%E4%BB%B6/">软件</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-star-o"> Tags</i></div><div class="tagcloud"><a href="/peaky/tags/%E6%B5%8F%E8%A7%88%E5%99%A8/" style="font-size: 15px;">浏览器</a> <a href="/peaky/tags/Hexo/" style="font-size: 15px;">Hexo</a> <a href="/peaky/tags/%E6%95%99%E7%A8%8B/" style="font-size: 15px;">教程</a> <a href="/peaky/tags/Linux/" style="font-size: 15px;">Linux</a> <a href="/peaky/tags/%E5%AD%A6%E4%B9%A0/" style="font-size: 15px;">学习</a> <a href="/peaky/tags/MIUI/" style="font-size: 15px;">MIUI</a> <a href="/peaky/tags/MarkDown/" style="font-size: 15px;">MarkDown</a> <a href="/peaky/tags/%E8%AF%AD%E6%B3%95/" style="font-size: 15px;">语法</a> <a href="/peaky/tags/python/" style="font-size: 15px;">python</a> <a href="/peaky/tags/RL/" style="font-size: 15px;">RL</a> <a href="/peaky/tags/%E7%BC%96%E7%A8%8B/" style="font-size: 15px;">编程</a> <a href="/peaky/tags/%E5%AF%BC%E8%88%AA/" style="font-size: 15px;">导航</a> <a href="/peaky/tags/%E8%BD%AF%E4%BB%B6/" style="font-size: 15px;">软件</a> <a href="/peaky/tags/%E7%94%B5%E8%84%91/" style="font-size: 15px;">电脑</a> <a href="/peaky/tags/%E6%97%A5%E5%B8%B8/" style="font-size: 15px;">日常</a> <a href="/peaky/tags/springboot/" style="font-size: 15px;">springboot</a></div></div><div class="widget"><div class="widget-title"><i class="fa fa-file-o"> Letzte</i></div><ul class="post-list"><li class="post-list-item"><a class="post-list-link" href="/peaky/2022/03/03/4a17b156.html">Hello World</a></li><li class="post-list-item"><a class="post-list-link" href="/peaky/2022/03/01/8d39085b.html">Linux部署git与Nodejs</a></li><li class="post-list-item"><a class="post-list-link" href="/peaky/2021/08/27/87d44b9d.html">MIUI教程</a></li><li class="post-list-item"><a class="post-list-link" href="/peaky/2021/06/01/9211474f.html">linux commd</a></li><li class="post-list-item"><a class="post-list-link" href="/peaky/2021/05/05/a54293f2.html">Springboot笔记</a></li><li class="post-list-item"><a class="post-list-link" href="/peaky/2021/03/15/217f21be.html">DRL笔记</a></li><li class="post-list-item"><a class="post-list-link" href="/peaky/2021/03/01/514027fa.html">Hexo部署到阿里云OSS上</a></li><li class="post-list-item"><a class="post-list-link" href="/peaky/2021/01/28/ef00c80b.html">LayUI入门</a></li><li class="post-list-item"><a class="post-list-link" href="/peaky/2021/01/23/44737761.html">小米手机刷机入门指北（小白向）</a></li><li class="post-list-item"><a class="post-list-link" href="/peaky/2021/01/20/a2f6ccf6.html">layuiAdmin pro v1.x 【单页版】开发者文档</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-external-link"> Blogroll</i></div><ul></ul><a href="http://www.example1.com/" title="相关链接" target="_blank">相关链接</a><ul></ul><a href="http://www.example2.com/" title="相关链接" target="_blank">相关链接</a><ul></ul><a href="http://www.example3.com/" title="相关链接" target="_blank">相关链接</a></div></div></div><div class="pure-u-1 pure-u-md-3-4"><div id="footer">Copyright © 2022 <a href="/peaky/." rel="nofollow">NoteBook.</a> Powered by<a rel="nofollow" target="_blank" href="https://hexo.io"> Hexo.</a><a rel="nofollow" target="_blank" href="https://github.com/tufu9441/maupassant-hexo"> Theme</a> by<a rel="nofollow" target="_blank" href="https://github.com/pagecho"> Cho.</a></div></div></div><a class="show" id="rocket" href="#top"></a><script type="text/javascript" src="/peaky/js/totop.js?v=1.0.0" async></script><script type="text/javascript" src="//cdn.jsdelivr.net/gh/fancyapps/fancybox/dist/jquery.fancybox.min.js" async></script><script type="text/javascript" src="/peaky/js/fancybox.js?v=1.0.0" async></script><link rel="stylesheet" type="text/css" href="//cdn.jsdelivr.net/gh/fancyapps/fancybox/dist/jquery.fancybox.min.css"><link rel="stylesheet" type="text/css" href="/peaky/css/search.css?v=1.0.0"><script type="text/javascript" src="/peaky/js/search.js?v=1.0.0"></script><script>var search_path = 'search.xml';
if (search_path.length == 0) {
   search_path = 'search.xml';
}
var path = '/peaky/' + search_path;
searchFunc(path, 'local-search-input', 'local-search-result');
</script><script type="text/javascript" src="/js/copycode.js" successtext="Copy Successed!"></script><link rel="stylesheet" type="text/css" href="/css/copycode.css"><script type="text/javascript" src="/peaky/js/codeblock-resizer.js?v=1.0.0"></script><script type="text/javascript" src="/peaky/js/smartresize.js?v=1.0.0"></script></div></body></html>